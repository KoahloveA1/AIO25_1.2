import streamlit as st
import os
import torch

from src.chatbot import LLMClient, MemoryRetrival, GenerateData
import datetime
import json
import os



def create_json_file_if_not_exists(path: str, initial_data: dict = None):
    """
    Táº¡o file JSON náº¿u chÆ°a tá»“n táº¡i.

    Args:
        path (str): ÄÆ°á»ng dáº«n tá»›i file JSON.
        initial_data (dict): Dá»¯ liá»‡u khá»Ÿi táº¡o náº¿u cáº§n. Máº·c Ä‘á»‹nh lÃ  dict rá»—ng.
    """
    check = False
    if not os.path.exists(path):
        # Táº¡o thÆ° má»¥c náº¿u chÆ°a tá»“n táº¡i
        os.makedirs(os.path.dirname(path), exist_ok=True)
        check = True

        # Ghi file JSON rá»—ng hoáº·c cÃ³ ná»™i dung khá»Ÿi táº¡o
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(initial_data or {}, f, ensure_ascii=False, indent=4)
        print(f"âœ… Created file: {path}")
    else:
        print(f"âœ… File already exists: {path}")
    return check



model_path = os.path.abspath('models/vinallama-7b-chat_q5_0.gguf')

get_data = GenerateData("intfloat/multilingual-e5-base", 'data_pdf')


# get_data.build_faiss_index()
"""
Äá»ƒ sá»­ dá»¥ng chatbot vui lÃ²ng truyá»n cÃ¢u há»i vÃ o query
Äiá»n tÃªn Ä‘á»ƒ chatbot ghi nhá»› thÃ´ng tin ngÆ°á»i dÃ¹ng
Náº¿u Ä‘Ã¢y lÃ  láº§n Ä‘áº§u sá»­ dá»¥ng chatbot thÃ¬ truyá»n firstTime = True
Náº¿u khÃ´ng thÃ¬ truyá»n firstTime = False
Náº¿u muá»‘n tÃ³m táº¯t láº¡i thÃ´ng tin ngÆ°á»i dÃ¹ng thÃ¬ truyá»n summary = True
Náº¿u muá»‘n quÃªn thÃ´ng tin ngÆ°á»i dÃ¹ng thÃ¬ truyá»n apply_forget = True
"""
def load_llm_client(user_name: str, model_path: str):
    return LLMClient(user_name, model_path)

def load_memory_retrival(user_name: str):
    return MemoryRetrival(
    "intfloat/multilingual-e5-base",  # âœ… KhÃ´ng cÃ³ tÃªn tham sá»‘
    5,
    200,
    user_name,
    'data/test_json.json'
    )

def test_llm_conversation(firstTime = False, query = '', user_name ='', summary = False, apply_forget = False):


  if create_json_file_if_not_exists("data/test_json.json"):
    print("Dá»¯ liá»‡u chÆ°a cÃ³")
    print("Vui lÃ²ng Ä‘á»£i há»‡ thá»‘ng khá»Ÿi táº¡o")
    return


  llm_client = load_llm_client(user_name, model_path)
  cur_date = datetime.datetime.now().strftime("%Y-%m-%d")
  memory_retrival = load_memory_retrival(user_name)

  response = ''

  # DÃ¹ng cÆ¡ cháº¿ tÃ³m táº¯t láº¡i thÃ´ng tin ngÆ°á»i dÃ¹ng sau khi tÃ³m táº¯t láº¡i thÃ´ng tin ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ refresh láº¡i index
  if summary:
    print('TÃ³m táº¯t')
    llm_client.summary_user()
    memory_retrival.init_memory_index('data/test_json.json', 'index_storage', user_name, cur_date)
    return
  # DÃ¹ng cÆ¡ cháº¿ quÃªn thÃ´ng tin ngÆ°á»i dÃ¹ng sau khi tÃ³m táº¯t láº¡i thÃ´ng tin ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ refresh láº¡i index
  if apply_forget:
    llm_client.apply_forget()
    return

  # Láº§n Ä‘áº§u chat vá»›i chatbot
  if firstTime:
    # memory_loader = MemoryForgetLoader('data/test_json.json')

    print("HELLO")
    data = get_data.query(query)

    input = [
        'data',
        'query'
    ]
    print(data)
    context = """
    Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh vÃ  thÃ¢n thiá»‡n.

    ThÃ´ng tin liÃªn quan Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i: {data}

    CÃ¢u há»i tá»« ngÆ°á»i dÃ¹ng:
    {query}

    Dá»±a trÃªn thÃ´ng tin vÃ  dá»¯ kiá»‡n á»Ÿ trÃªn, hÃ£y Ä‘Æ°a ra cÃ¢u tráº£ lá»i phÃ¹ há»£p.
    """
    # Nháº­n pháº£n há»“i vá»›i Ä‘Ã¢y lÃ  láº§n Ä‘áº§u tiÃªn sá»­ sá»¥ng chatbot
    template = llm_client.create_template(input, context)
    llm_client.build_model_with_template(template)

    response = llm_client.generate_output(query, data, personal_info='')
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()
    # Khá»Ÿi táº¡o má»™t index (tÆ°Æ¡ng Ä‘Æ°Æ¡ng) 1 vector db dÃ¹ng Ä‘á»ƒ lÆ°u trá»¯ file pdf
    memory_retrival.init_memory_index('data/test_json.json', 'index_storage', user_name, cur_date)
    print("DEBUGGING")


  else:
    data = get_data.query(query)

    index = memory_retrival.load_memory_index(f'index_storage/{user_name}')

    personal_info = memory_retrival.search_memory(query, index, cur_date)
    input = [
        'personal_info',
        'data',
        'query'
    ]

    context = """
    Báº¡n lÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh vÃ  thÃ¢n thiá»‡n.

    ThÃ´ng tin cÃ¡ nhÃ¢n ngÆ°á»i dÃ¹ng: {personal_info}
    ThÃ´ng tin liÃªn quan Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i: {data}

    CÃ¢u há»i tá»« ngÆ°á»i dÃ¹ng:
    {query}

    Dá»±a trÃªn thÃ´ng tin cÃ¡ nhÃ¢n vÃ  dá»¯ kiá»‡n á»Ÿ trÃªn, hÃ£y Ä‘Æ°a ra cÃ¢u tráº£ lá»i phÃ¹ há»£p, Ä‘á»“ng cáº£m vÃ  há»¯u Ã­ch. Náº¿u thÃ´ng tin ngÆ°á»i khÃ´ng há»¯u Ã­ch hÃ£y bá» qua.
    """
    template = llm_client.create_template(input, context)
    llm_client.build_model_with_template(template)
    response = llm_client.generate_output(query, data, personal_info)
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()
    # Refresh láº¡i index má»—i láº§n cÃ³ dá»¯ liá»‡u vá» Ä‘oáº¡n chat má»›i Ä‘Æ°á»£c thÃªm vÃ o (cÃ³ thá»ƒ tiáº¿t kiá»‡m chi phÃ­ thÃ¬ refesh sau má»™t khoáº£ng thá»i gian nÃ o Ä‘Ã³)
    memory_retrival.init_memory_index('data/test_json.json', 'index_storage', user_name, cur_date)




  return response if response else 'KhÃ´ng cÃ³ cÃ¢u tráº£ lá»i'




if __name__ == '__main__':
   
    st.title("PDF RAG CHATBOT")

    st.markdown("""
    **á»¨ng dá»¥ng AI giÃºp báº¡n há»i Ä‘Ã¡p trá»±c tiáº¿p vá»›i ná»™i dung tÃ i liá»‡u PDF báº±ng tiáº¿ng Viá»‡t**
    *CÃ¡ch sá»­ dá»¥ng Ä‘Æ¡n giáº£n:**
    1. **Upload PDF** Chá»n file PDF tá»« mÃ¡y tÃ­nh vÃ  nháº¥n "Xá»­ lÃ½ PDF"
    2. **Äáº·t cÃ¢u há»i** Nháº­p cÃ¢u há»i vá» ná»™i dung tÃ i liá»‡u vÃ  nháº­n cÃ¢u tráº£ lá»i ngay láº­p tá»©c
    """)

    uploaded_file = st.file_uploader("Upload file PDF", type="pdf")

    if uploaded_file is not None:
        # Táº¡o thÆ° má»¥c lÆ°u file
        save_dir = os.path.join(os.getcwd(), "data_pdf")
        os.makedirs(save_dir, exist_ok=True)

        # Táº¡o Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ Ä‘á»ƒ lÆ°u
        file_path = os.path.join(save_dir, uploaded_file.name)

        # LÆ°u file nhá»‹ phÃ¢n
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        st.success(f"ÄÃ£ lÆ°u file vÃ o: {file_path}")



    handle_pdf = st.button("Xá»­ lÃ½ PDF")
    if handle_pdf:
        get_data.build_faiss_index()

    with st.form("user_input_form"):
        user_name = st.text_input("ğŸ‘¤ Nháº­p tÃªn cá»§a báº¡n:", placeholder="VD: LÃª Khoa")
        query = st.text_area("ğŸ’¬ Äáº·t cÃ¢u há»i:", placeholder="TÃ´i nÃªn há»c gÃ¬ Ä‘á»ƒ giá»i AI?")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            firstTime = st.checkbox("Láº§n Ä‘áº§u sá»­ dá»¥ng", value=False)
        with col2:
            summary = st.checkbox("TÃ³m táº¯t thÃ´ng tin", value=False)
        with col3:
            apply_forget = st.checkbox("QuÃªn thÃ´ng tin", value=False)

        submitted = st.form_submit_button("ğŸš€ Gá»­i cÃ¢u há»i")

    if submitted:
        if user_name.strip() == "" or query.strip() == "":
            st.warning("âš ï¸ Vui lÃ²ng nháº­p Ä‘áº§y Ä‘á»§ tÃªn vÃ  cÃ¢u há»i.")
        else:
            with st.spinner("ğŸ’¡ Äang suy nghÄ©..."):
                try:
                    answer = test_llm_conversation(
                        firstTime=firstTime,
                        query=query,
                        user_name=user_name,
                        summary=summary,
                        apply_forget=apply_forget
                    )
                    st.success("âœ… Trá»£ lÃ½ tráº£ lá»i:")
                    st.write(answer)
                    
                except Exception as e:
                    st.error(f"âŒ CÃ³ lá»—i xáº£y ra: {e}")