import streamlit as st
import os
import torch

from src.chatbot import LLMClient, MemoryRetrival, GenerateData
import datetime
import json
import os



def create_json_file_if_not_exists(path: str, initial_data: dict = None):
    """
    T·∫°o file JSON n·∫øu ch∆∞a t·ªìn t·∫°i.

    Args:
        path (str): ƒê∆∞·ªùng d·∫´n t·ªõi file JSON.
        initial_data (dict): D·ªØ li·ªáu kh·ªüi t·∫°o n·∫øu c·∫ßn. M·∫∑c ƒë·ªãnh l√† dict r·ªóng.
    """
    check = False
    if not os.path.exists(path):
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(os.path.dirname(path), exist_ok=True)
        check = True

        # Ghi file JSON r·ªóng ho·∫∑c c√≥ n·ªôi dung kh·ªüi t·∫°o
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(initial_data or {}, f, ensure_ascii=False, indent=4)
        print(f"‚úÖ Created file: {path}")
    else:
        print(f"‚úÖ File already exists: {path}")
    return check



model_path = os.path.abspath('models/vinallama-7b-chat_q5_0.gguf')

get_data = GenerateData("intfloat/multilingual-e5-base", 'data_pdf')


# get_data.build_faiss_index()
"""
ƒê·ªÉ s·ª≠ d·ª•ng chatbot vui l√≤ng truy·ªÅn c√¢u h·ªèi v√†o query
ƒêi·ªÅn t√™n ƒë·ªÉ chatbot ghi nh·ªõ th√¥ng tin ng∆∞·ªùi d√πng
N·∫øu ƒë√¢y l√† l·∫ßn ƒë·∫ßu s·ª≠ d·ª•ng chatbot th√¨ truy·ªÅn firstTime = True
N·∫øu kh√¥ng th√¨ truy·ªÅn firstTime = False
N·∫øu mu·ªën t√≥m t·∫Øt l·∫°i th√¥ng tin ng∆∞·ªùi d√πng th√¨ truy·ªÅn summary = True
N·∫øu mu·ªën qu√™n th√¥ng tin ng∆∞·ªùi d√πng th√¨ truy·ªÅn apply_forget = True
"""
def load_llm_client(user_name: str, model_path: str):
    return LLMClient(user_name, model_path)

def load_memory_retrival(user_name: str):
    return MemoryRetrival(
    "intfloat/multilingual-e5-base",  # ‚úÖ Kh√¥ng c√≥ t√™n tham s·ªë
    5,
    200,
    user_name,
    'data/test_json.json'
    )



def test_llm_conversation(firstTime = False, query = '', user_name ='', summary = False, apply_forget = False):


  if create_json_file_if_not_exists("data/test_json.json"):
    print("D·ªØ li·ªáu ch∆∞a c√≥")
    print("Vui l√≤ng ƒë·ª£i h·ªá th·ªëng kh·ªüi t·∫°o")
    return


  llm_client = load_llm_client(user_name, model_path)
  cur_date = datetime.datetime.now().strftime("%Y-%m-%d")
  memory_retrival = load_memory_retrival(user_name)

  response = ''

  # D√πng c∆° ch·∫ø t√≥m t·∫Øt l·∫°i th√¥ng tin ng∆∞·ªùi d√πng sau khi t√≥m t·∫Øt l·∫°i th√¥ng tin ng∆∞·ªùi d√πng c√≥ th·ªÉ refresh l·∫°i index
  if summary:
    print('T√≥m t·∫Øt')
    llm_client.summary_user()
    memory_retrival.init_memory_index('data/test_json.json', 'index_storage', user_name, cur_date)
    return
  # D√πng c∆° ch·∫ø qu√™n th√¥ng tin ng∆∞·ªùi d√πng sau khi t√≥m t·∫Øt l·∫°i th√¥ng tin ng∆∞·ªùi d√πng c√≥ th·ªÉ refresh l·∫°i index
  if apply_forget:
    llm_client.apply_forget()
    return

  # L·∫ßn ƒë·∫ßu chat v·ªõi chatbot
  if firstTime:
    # memory_loader = MemoryForgetLoader('data/test_json.json')

    print("HELLO")
    data = get_data.query(query)

    input = [
        'data',
        'query'
    ]
    print(data)
    context = """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh v√† th√¢n thi·ªán.

    Th√¥ng tin li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi: {data}

    C√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng:
    {query}

    D·ª±a tr√™n th√¥ng tin v√† d·ªØ ki·ªán ·ªü tr√™n, h√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ph√π h·ª£p.
    """
    # Nh·∫≠n ph·∫£n h·ªìi v·ªõi ƒë√¢y l√† l·∫ßn ƒë·∫ßu ti√™n s·ª≠ s·ª•ng chatbot
    template = llm_client.create_template(input, context)
    llm_client.build_model_with_template(template)

    response = llm_client.generate_output(query, data, personal_info='')
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()
    # Kh·ªüi t·∫°o m·ªôt index (t∆∞∆°ng ƒë∆∞∆°ng) 1 vector db d√πng ƒë·ªÉ l∆∞u tr·ªØ file pdf
    memory_retrival.init_memory_index('data/test_json.json', 'index_storage', user_name, cur_date)
    print("DEBUGGING")


  else:
    data = get_data.query(query)

    index = memory_retrival.load_memory_index(f'index_storage/{user_name}')

    personal_info = memory_retrival.search_memory(query, index, cur_date)
    input = [
        'personal_info',
        'data',
        'query'
    ]

    context = """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh v√† th√¢n thi·ªán.

    Th√¥ng tin c√° nh√¢n ng∆∞·ªùi d√πng: {personal_info}
    Th√¥ng tin li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi: {data}

    C√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng:
    {query}

    D·ª±a tr√™n th√¥ng tin c√° nh√¢n v√† d·ªØ ki·ªán ·ªü tr√™n, h√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ph√π h·ª£p, ƒë·ªìng c·∫£m v√† h·ªØu √≠ch. N·∫øu th√¥ng tin ng∆∞·ªùi kh√¥ng h·ªØu √≠ch h√£y b·ªè qua.
    """


    # input = [
    #     'personal_info',
    #     'data',
    #     'query'
    # ]

    # context = """
    # B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, th√¢n thi·ªán v√† bi·∫øt ƒë·ªìng c·∫£m.

    # Th√¥ng tin c√° nh√¢n ng∆∞·ªùi d√πng (n·∫øu c√≥):  
    # {personal_info}

    # Ng·ªØ c·∫£nh ho·∫∑c d·ªØ ki·ªán li√™n quan:  
    # {data}

    # Ng∆∞·ªùi d√πng ƒëang th·ªÉ hi·ªán c·∫£m x√∫c ho·∫∑c chia s·∫ª n·ªôi t√¢m nh∆∞ sau:  
    # {query}

    # H√£y ph·∫£n h·ªìi m·ªôt c√°ch nh·∫π nh√†ng, th·∫•u hi·ªÉu v√† h·ªØu √≠ch. T·∫≠p trung v√†o vi·ªác ƒë·ªìng c·∫£m, khuy·∫øn kh√≠ch ho·∫∑c h∆∞·ªõng d·∫´n tinh t·∫ø n·∫øu ph√π h·ª£p.  
    # **N·∫øu n·ªôi dung mang t√≠nh c·∫£m x√∫c m√† kh√¥ng y√™u c·∫ßu th√¥ng tin k·ªπ thu·∫≠t hay t√†i li·ªáu c·ª• th·ªÉ (nh∆∞ PDF), h√£y tr√°nh ƒë·ªÅ c·∫≠p ƒë·∫øn file hay t√†i li·ªáu.**  
    # M·ª•c ti√™u l√† khi·∫øn ng∆∞·ªùi d√πng c·∫£m th·∫•y ƒë∆∞·ª£c l·∫Øng nghe v√† h·ªó tr·ª£. N·∫øu c√¢u h·ªèi b·∫°n c√≥ th·ªÉ tr·∫£ l·ªùi ƒë∆∞·ª£c th√¨ xin h√£y tr·∫£ l·ªùi.
    # """

    # input = [
    #     'personal_info',
    #     'query',
    #     'context_data'
    # ]

    # context = """
    # B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, th√¢n thi·ªán v√† bi·∫øt c√°ch g·ª£i √Ω ph√π h·ª£p v·ªõi ng∆∞·ªùi d√πng.

    # Th√¥ng tin c√° nh√¢n ng∆∞·ªùi d√πng (n·∫øu c√≥):  
    # {personal_info}

    # Th√¥ng tin ng·ªØ c·∫£nh ho·∫∑c d·ªØ ki·ªán li√™n quan (n·∫øu c√≥):  
    # {data}

    # Ng∆∞·ªùi d√πng chia s·∫ª s·ªü th√≠ch ho·∫∑c th·∫Øc m·∫Øc nh∆∞ sau:  
    # {query}

    # H√£y ƒë∆∞a ra ph·∫£n h·ªìi th√¢n thi·ªán, g·∫ßn g≈©i v√† h·ªØu √≠ch. N·∫øu ng∆∞·ªùi d√πng th·ªÉ hi·ªán s·ªü th√≠ch (nh∆∞ du l·ªãch, ƒë·ªçc s√°ch, kh√°m ph√°...), h√£y ƒë·ªÅ xu·∫•t m·ªôt s·ªë √Ω t∆∞·ªüng ho·∫∑c h∆∞·ªõng d·∫´n ph√π h·ª£p v·ªõi s·ªü th√≠ch ƒë√≥.

    # N·∫øu l√† m·ªôt c√¢u h·ªèi ho·∫∑c ƒë·ªÅ ngh·ªã nh·∫π nh√†ng, h√£y ph·∫£n h·ªìi ng·∫Øn g·ªçn, truy·ªÅn c·∫£m h·ª©ng v√† ƒë√∫ng tr·ªçng t√¢m. M·ª•c ti√™u l√† gi√∫p ng∆∞·ªùi d√πng c·∫£m th·∫•y ƒë∆∞·ª£c ƒë·ªìng h√†nh v√† h·ªó tr·ª£ theo c√°ch t√≠ch c·ª±c.
    # """
    template = llm_client.create_template(input, context)
    llm_client.build_model_with_template(template)
    response = llm_client.generate_output(query, data, personal_info)
    torch.cuda.empty_cache()
    torch.cuda.ipc_collect()
    # Refresh l·∫°i index m·ªói l·∫ßn c√≥ d·ªØ li·ªáu v·ªÅ ƒëo·∫°n chat m·ªõi ƒë∆∞·ª£c th√™m v√†o (c√≥ th·ªÉ ti·∫øt ki·ªám chi ph√≠ th√¨ refesh sau m·ªôt kho·∫£ng th·ªùi gian n√†o ƒë√≥)
    memory_retrival.init_memory_index('data/test_json.json', 'index_storage', user_name, cur_date)




  return response if response else 'Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi'




if __name__ == '__main__':
   
    st.title("PDF RAG CHATBOT")

    st.markdown("""
    **·ª®ng d·ª•ng AI gi√∫p b·∫°n h·ªèi ƒë√°p tr·ª±c ti·∫øp v·ªõi n·ªôi dung t√†i li·ªáu PDF b·∫±ng ti·∫øng Vi·ªát**
    *C√°ch s·ª≠ d·ª•ng ƒë∆°n gi·∫£n:**
    1. **Upload PDF** Ch·ªçn file PDF t·ª´ m√°y t√≠nh v√† nh·∫•n "X·ª≠ l√Ω PDF"
    2. **ƒê·∫∑t c√¢u h·ªèi** Nh·∫≠p c√¢u h·ªèi v·ªÅ n·ªôi dung t√†i li·ªáu v√† nh·∫≠n c√¢u tr·∫£ l·ªùi ngay l·∫≠p t·ª©c
    """)

    uploaded_file = st.file_uploader("Upload file PDF", type="pdf")

    if uploaded_file is not None:
        # T·∫°o th∆∞ m·ª•c l∆∞u file
        save_dir = os.path.join(os.getcwd(), "data_pdf")
        os.makedirs(save_dir, exist_ok=True)

        # T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·ªÉ l∆∞u
        file_path = os.path.join(save_dir, uploaded_file.name)

        # L∆∞u file nh·ªã ph√¢n
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        st.success(f"ƒê√£ l∆∞u file v√†o: {file_path}")



    handle_pdf = st.button("X·ª≠ l√Ω PDF")
    if handle_pdf:
        get_data.build_faiss_index()

    with st.form("user_input_form"):
        user_name = st.text_input("üë§ Nh·∫≠p t√™n c·ªßa b·∫°n:", placeholder="VD: L√™ Khoa")
        query = st.text_area("üí¨ ƒê·∫∑t c√¢u h·ªèi:", placeholder="T√¥i n√™n h·ªçc g√¨ ƒë·ªÉ gi·ªèi AI?")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            firstTime = st.checkbox("L·∫ßn ƒë·∫ßu s·ª≠ d·ª•ng", value=False)
        with col2:
            summary = st.checkbox("T√≥m t·∫Øt th√¥ng tin", value=False)
        with col3:
            apply_forget = st.checkbox("Qu√™n th√¥ng tin", value=False)

        submitted = st.form_submit_button("üöÄ G·ª≠i c√¢u h·ªèi")

    if submitted:
        if user_name.strip() == "" or query.strip() == "":
            st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p ƒë·∫ßy ƒë·ªß t√™n v√† c√¢u h·ªèi.")
        else:
            with st.spinner("üí° ƒêang suy nghƒ©..."):
                try:
                    answer = test_llm_conversation(
                        firstTime=firstTime,
                        query=query,
                        user_name=user_name,
                        summary=summary,
                        apply_forget=apply_forget
                    )
                    st.success("‚úÖ Tr·ª£ l√Ω tr·∫£ l·ªùi:")
                    st.write(answer)
                    
                except Exception as e:
                    st.error(f"‚ùå C√≥ l·ªói x·∫£y ra: {e}")