import copy
import json
import PyPDF2

from langchain_community.llms import LlamaCpp
from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler
from langchain.chains import LLMChain
from langchain.prompts import ChatPromptTemplate
from langchain_core.prompts import PromptTemplate
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document

import re
from typing import List, Tuple
import datetime
import os
import math
import random
import numpy as np




def clean_text(text: str) -> str:
    """
    Giá»¯ láº¡i cáº¥u trÃºc dáº¡ng Ä‘oáº¡n vÄƒn hoáº·c liá»‡t kÃª.
    XÃ³a khoáº£ng tráº¯ng Ä‘áº§u dÃ²ng, dÃ²ng trá»‘ng, vÃ  chuáº©n hÃ³a dÃ²ng liÃªn tá»¥c.
    """
    # TÃ¡ch tá»«ng dÃ²ng, loáº¡i bá» khoáº£ng tráº¯ng Ä‘áº§u/cuá»‘i tá»«ng dÃ²ng
    lines = [line.strip() for line in text.strip().splitlines()]

    # Loáº¡i bá» dÃ²ng trá»‘ng
    lines = [line for line in lines if line]

    # Náº¿u toÃ n bá»™ chá»‰ lÃ  má»™t Ä‘oáº¡n vÄƒn khÃ´ng xuá»‘ng dÃ²ng cÃ³ chá»§ Ã½, ná»‘i láº¡i má»™t dÃ²ng
    if all(not re.match(r'^[-â€¢*]|^[A-Z][a-z]+:', line) for line in lines):
        return ' '.join(lines)

    # NgÆ°á»£c láº¡i: giá»¯ xuá»‘ng dÃ²ng giá»¯a cÃ¡c Ä‘oáº¡n cÃ³ Ã½ nghÄ©a
    return '\n'.join(lines)


# Class khá»Ÿi táº¡o model llms
class LLMs:
    def __init__(self, model_id):
        self.__model = LlamaCpp(
            model_path=model_id,
            n_gpu_layers=5,             # âš ï¸ Giá»›i háº¡n an toÃ n cho 4GB (thá»­ 10â€“16 tÃ¹y model)
            n_ctx=4096,                  # âš ï¸ Giáº£m Ä‘á»ƒ tiáº¿t kiá»‡m VRAM (4096 náº¿u RAM Ä‘á»§)
            max_tokens=256,              # Giá»¯ nguyÃªn
            temperature=0.1,
            top_p=0.95,
            n_threads=8,                 # Phá»¥ thuá»™c sá»‘ CPU
            n_batch=64,                  # âš ï¸ Giáº£m batch size Ä‘á»ƒ giáº£m memory peak
            use_mmap=True,
            use_mlock=True,              # Giá»¯ model trong RAM
            verbose=True,
            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])
        )

    def build_llm(self, prompt_template: PromptTemplate):
        self.__llm_chain = LLMChain(llm=self.__model, prompt=prompt_template)
        return self.__llm_chain

    def get_model(self):
        return self.__model
    def generate_output(self, input_dict: dict):
        if not hasattr(self, '_LLMs__llm_chain'):
            raise RuntimeError("LLM chain not built. Call build_llm() first.")
        raw_output = self.__llm_chain.run(input_dict)
        return clean_text(raw_output)

    


# CÆ¡ cháº¿ quÃªn Ä‘Æ°á»£c xÃ¢y dá»±ng hÃ m log
def forgetting_curve(t, S):
    """
    Calculate the retention of information at time t based on the forgetting curve.

    :param t: Time elapsed since the information was learned (in days).
    :type t: float
    :param S: Strength of the memory.
    :type S: float
    :return: Retention of information at time t.
    :rtype: float
    Memory strength is a concept used in memory models to represent the durability or stability of a memory trace in the brain.
    In the context of the forgetting curve, memory strength (denoted as 'S') is a parameter that
    influences the rate at which information is forgotten.
    The higher the memory strength, the slower the rate of forgetting,
    and the longer the information is retained.
    """
    return math.exp(-t / 5*S)    





def get_docs_with_score(docs_with_score):
    docs=[]
    for doc, score in docs_with_score:
        doc.metadata["score"] = score
        docs.append(doc)
    return docs


def seperate_list(ls: List[int]) -> List[List[int]]:
    lists = []
    ls1 = [ls[0]]
    for i in range(1, len(ls)):
        if ls[i-1] + 1 == ls[i]:
            ls1.append(ls[i])
        else:
            lists.append(ls1)
            ls1 = [ls[i]]
    lists.append(ls1)
    return lists


def similarity_search_with_score_by_vector(
        self,
        embedding: List[float],
        k: int = 4,
        **kwargs
    ) -> List[Tuple[Document, float]]:
        scores, indices = self.index.search(np.array([embedding], dtype=np.float32), k)
        docs = []
        id_set = set()
        for j, i in enumerate(indices[0]):
            if i == -1:
                # This happens when not enough docs are returned.
                continue
            _id = self.index_to_docstore_id[i]
            doc = self.docstore.search(_id)
            id_set.add(i)
            docs_len = len(doc.page_content)
            for k in range(1, max(i, len(self.index_to_docstore_id)-i)):
                for l in [i+k, i-k]:
                    if 0 <= l < len(self.index_to_docstore_id):
                        _id0 = self.index_to_docstore_id[l]
                        doc0 = self.docstore.search(_id0)
                        # print(doc0.metadata)
                        # exit()
                        if docs_len + len(doc0.page_content) > self.chunk_size:
                            break
                        # print(doc0)
                        elif doc0.metadata["source"] == doc.metadata["source"]:
                            docs_len += len(doc0.page_content)
                            id_set.add(l)
        id_list = sorted(list(id_set))
        id_lists = seperate_list(id_list)
        for id_seq in id_lists:
            for id in id_seq:
                if id == id_seq[0]:
                    _id = self.index_to_docstore_id[id]
                    doc = self.docstore.search(_id)
                else:
                    _id0 = self.index_to_docstore_id[id]
                    doc0 = self.docstore.search(_id0)
                    doc.page_content += doc0.page_content
            if not isinstance(doc, Document):
                raise ValueError(f"Could not find document for id {_id}, got {doc}")
            docs.append((doc, scores[0][j]))
        return docs



class MemoryForgetLoader:
    # Khá»Ÿi táº¡o má»™t dict táº¡m Ä‘á»ƒ lÆ°u trÅ© cÃ¡c thÃ´ng tin.
    def __init__(self, file_path):
        self.file_path = file_path
        self.memory_bank = {}

    def _get_date_difference(self, date1: str, date2: str) -> int:
        date_format = "%Y-%m-%d"
        d1 = datetime.datetime.strptime(date1, date_format)
        d2 = datetime.datetime.strptime(date2, date_format)
        return (d2 - d1).days


    def write_memories(self, out_file):
        with open(out_file, "w", encoding="utf-8") as f:
            print(f'Successfully write to {out_file}')
            json.dump(self.memory_bank, f, ensure_ascii=False, indent=4)

    def load_memories(self, memory_file):
        # print(memory_file)
        with open(memory_file, "r", encoding="utf-8") as f:
            self.memory_bank = json.load(f)
    # HÃ m khá»Ÿi táº¡o vÃ  cáº­p nháº­t metadata vÃ o db cÃ³ kÃ¨m theo cÆ¡ cháº¿  suy giáº£m trÃ­ nhá»›.
    def update_forget_memory(self, name, cur_date):
        with open(self.file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        docs_about_user = []
        for user_name, info in data.items():
            self.memory_bank[user_name] = copy.deepcopy(info)
            if user_name != name:
                continue
            if 'history' not in info.keys():
                continue
            for date, dialog in info['history'].items():
                # Khá»Ÿi táº¡o máº£ng cÃ¡c Ä‘oáº¡n Ä‘á»‘i thoáº¡i sáº½ bá»‹ quÃªn cá»§a user trong date
                forget_index = []
                tmp_str = f"ÄÃ¢y lÃ  Ä‘oáº¡n Ä‘á»‘i thoáº¡i vÃ o {date} "
                for i, chat in enumerate(dialog):

                    # Khá»Ÿi táº¡o metadata hoáº·c láº¥y metadata Ä‘á»ƒ trÃ­ch xuáº¥t quÃªn cá»§a chabot vá» Ä‘oáº¡n há»™i thoáº¡i nÃ o Ä‘Ã³
                    memory_strength = chat.get('memory_strength', 1)
                    last_recall_date = chat.get('last_recall_date', date)
                    memory_id = chat.get('memory_id', f'{user_name}_{date}_{i}')
                    query = f"[|User|]: {chat['query']}"
                    response = f"[|AI|]: {chat['response']}"
                    tmp_str += query + response
                    metadata = {
                        'memory_strength': memory_strength,
                        'last_recall_date': last_recall_date,
                        'memory_id': memory_id
                    }
                    self.memory_bank[user_name]['history'][date][i].update(metadata)

                    diff_date = self._get_date_difference(last_recall_date, cur_date)
                    forget_probability = forgetting_curve(diff_date, memory_strength)
                    # Thá»­ kháº£ nÄƒng liá»‡u cÃ³ quÃªn hay khÃ´ng?
                    if random.random() > forget_probability:
                        forget_index.append(i)
                    else:
                        # LÆ°u thÃ´ng tin ngÆ°á»i dÃ¹ng Ä‘á»ƒ táº¡o documents vá»›i metadata Ä‘á»ƒ dá»… retrival
                        docs_about_user.append(Document(page_content=tmp_str,metadata=metadata))
                if len(forget_index) > 0:
                    forget_index.sort(reverse=True)
                    for idd in forget_index:
                        self.memory_bank[user_name]['history'][date].pop(idd)
                        print(f'Delete convestion of {user_name} on {date}')
                if len(self.memory_bank[user_name]['history'][date]) == 0:
                    self.memory_bank[user_name]['history'].pop(date)
                    self.memory_bank[user_name]['summary'].pop(date)

                if 'summary' in info.keys():
                    if date in self.memory_bank[user_name]['summary'].keys():
                        summary = f"ÄÃ¢y lÃ  tÃ³m táº¯t vÃ o ngÃ y {date}"
                        summary += data[user_name]['summary'][date]['content']
                        memory_strength = self.memory_bank[user_name]['summary'][date].get('memory_strength',1)
                        last_recall_date = self.memory_bank[user_name]["summary"][date].get('last_recall_date',date)
                        metadata = {
                            'memory_strength':memory_strength,
                            'memory_id':f'{user_name}_{date}_summary',
                            'last_recall_date':last_recall_date,"source":f'{user_name}_{date}_summary'
                        }
                        self.memory_bank[user_name]['summary'][date].update(metadata)
                        docs_about_user.append(Document(page_content=summary,metadata=metadata))
                # if 'overall_history' in info.keys():
                #     metadata = {
                #         'overall_history' : user_name
                #     }
                #     docs_about_user.append(Document(page_content=data[user_name]['overall_history'], metadata=metadata))
                # if 'overall_personality' in info.keys():
                #     metadata = {
                #         'overall_personality' : user_name
                #     }
                #     docs_about_user.append(Document(page_content=data[user_name]['overall_personality'], metadata=metadata))
        self.write_memories(self.file_path)
        return docs_about_user

    def update_memory_when_searched(self, recalled_memos,user,cur_date):
        for recalled in recalled_memos:
            recalled_id = recalled.metadata['memory_id']
            recalled_date = recalled_id.split('_')[1]
            for i,memory in enumerate(self.memory_bank[user]['history'][recalled_date]):
                if memory['memory_id'] == recalled_id:
                    self.memory_bank[user]['history'][recalled_date][i]['memory_strength'] += 1
                    self.memory_bank[user]['history'][recalled_date][i]['last_recall_date'] = cur_date
                    break



# Class khá»Ÿi táº¡o má»™t instance cÃ³ kháº£ nÄƒng truy xuáº¥t bá»™ nhá»›.
from types import MethodType
class MemoryRetrival:
    def __init__(self, embedding_name, top_k, chunk_size, user, file_path):
        self.embedding_model = HuggingFaceEmbeddings(model_name=embedding_name)

        self.top_k = top_k
        self.chunk_size = chunk_size
        self.user = user
        self.memory_path = file_path

    def helper_load_file(self, file_name: str, user_name: str, cur_date):
        loader = MemoryForgetLoader(file_name)
        docs = loader.update_forget_memory(user_name,cur_date)
        splitter = RecursiveCharacterTextSplitter()
        docs = splitter.split_documents(docs)
        return docs, loader

    def init_memory_index(self, file_name: str, saving_path: str, user_name: str, cur_date: str):
        # Load vÃ  chunk vÄƒn báº£n
        docs, self.memory_loader = self.helper_load_file(file_name, user_name, cur_date)

        if not docs:
            print("âŒ KhÃ´ng cÃ³ tÃ i liá»‡u nÃ o Ä‘Æ°á»£c táº¡o.")
            return

        # Táº¡o FAISS index tá»« tÃ i liá»‡u
        vector_store = FAISS.from_documents(docs, self.embedding_model)

        # Äáº£m báº£o thÆ° má»¥c tá»“n táº¡i
        os.makedirs(saving_path, exist_ok=True)

        # Äáº·t tÃªn file FAISS theo user_name
        file_id = user_name or f"memory_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        full_path = os.path.join(saving_path, file_id)

        # LÆ°u FAISS
        vector_store.save_local(full_path)
        print(f"ğŸ“¦ FAISS index Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {full_path}")


    def load_memory_index(self, vs_path: str):
        """Táº£i FAISS index Ä‘Ã£ lÆ°u tá»« vs_path"""
        cur_date = datetime.datetime.now().strftime("%Y-%m-%d")
        docs, self.memory_loader = self.helper_load_file(self.memory_path, self.user, cur_date)
        vector_store = FAISS.load_local(
            vs_path,
            self.embedding_model,
            allow_dangerous_deserialization=True
        )
        print("ğŸ“Š FAISS index info:")
        print(" - Total docs in index:", len(vector_store.index_to_docstore_id))
        print(" - Index dimension:", vector_store.index.d)
        print(" - Query vector dimension:", len(self.embedding_model.embed_query("So sÃ¡nh esp32 vÃ  arduino")))
        # GÃ¡n hÃ m tÃ¬m kiáº¿m custom náº¿u cÃ³ (tá»‘i Æ°u ghÃ©p chunk)
        FAISS.similarity_search_with_score_by_vector = MethodType(similarity_search_with_score_by_vector, vector_store)

        # GÃ¡n chunk_size cho vector_store Ä‘á»ƒ sá»­ dá»¥ng trong search náº¿u cáº§n
        vector_store.chunk_size = self.chunk_size

        print(f"âœ… ÄÃ£ load memory index tá»«: {vs_path}")
        return vector_store


    def search_memory(self, query: str, vector_store, cur_date=''):
        print("ğŸ“Œ Query:", query)
        print("ğŸ“Œ Query embedding:", self.embedding_model.embed_query(query)[:10])  # in 10 sá»‘ Ä‘áº§u

        # Test truy xuáº¥t
        test_docs = vector_store.similarity_search(query, k=3)
        print("ğŸ“„ Top retrieved docs:")
        for i, d in enumerate(test_docs):
            print(f"{i+1}. {d.page_content[:100]}...")  # in 100 kÃ½ tá»± Ä‘áº§u má»—i doc
        related_docs_with_score = vector_store.similarity_search_with_score(query, k=self.top_k)
        related_docs = get_docs_with_score(related_docs_with_score)

        # âœ… TÃ¡ch ngÃ y tá»« memory_id Ä‘á»ƒ sáº¯p theo ngÃ y + memory_id
        def extract_date(doc):
            memory_id = doc.metadata["memory_id"]
            return memory_id.split("_")[1]  # vÃ­ dá»¥: "2025-06-25"

        related_docs = sorted(
            related_docs,
            key=lambda x: (extract_date(x), x.metadata["memory_id"])
        )

        pre_date = ''
        date_docs = []
        dates = []
        cur_date = cur_date if cur_date else datetime.date.today().strftime("%Y-%m-%d")

        for doc in related_docs:
            date_str = extract_date(doc)
            doc.page_content = doc.page_content.replace(f'ÄÃ¢y lÃ  Ä‘oáº¡n Ä‘á»‘i thoáº¡i vÃ o {date_str}ï¼š', '').strip()

            if date_str != pre_date:
                date_docs.append(doc.page_content)
                pre_date = date_str
                dates.append(pre_date)
            else:
                date_docs[-1] += f'\n{doc.page_content}'

        self.memory_loader.update_memory_when_searched(related_docs, user=self.user, cur_date=cur_date)
        self.save_updated_memory()
        return date_docs, ', '.join(dates)

    def save_updated_memory(self):
        self.memory_loader.write_memories(self.memory_path)#.replace('.json','_forget_format.json'))



class LLMClientSimple:
    def __init__(self, llm_chain):
        self.llm_chain = llm_chain  # LLMChain tá»« mÃ´ hÃ¬nh local Ä‘Ã£ Ä‘Æ°á»£c build

    def generate_text_simple(self, prompt, prompt_num=1, language='vi'):
        """
        Sinh vÄƒn báº£n tá»« local model vá»›i prompt Ä‘áº§u vÃ o.
        prompt_num khÃ´ng Ã¡p dá»¥ng vá»›i local LLM, chá»‰ giá»¯ láº¡i cho tÆ°Æ¡ng thÃ­ch.
        """
        try:
            result = self.llm_chain.invoke({"text": prompt})
            # Náº¿u LLMChain tráº£ vá» {'text': "..."} thÃ¬ chá»‰ láº¥y pháº§n ná»™i dung
            return result.get("text", "") if isinstance(result, dict) else result
        except Exception as e:
            print(f"Lá»—i khi sinh vÄƒn báº£n tá»« local model: {e}")
            return ""

# Táº¡o prompt cho tÃ³m táº¯t

def summarize_content_prompt(content, user_name, bot_name):
    prompt = "HÃ£y tÃ³m táº¯t ná»™i dung cuá»™c há»™i thoáº¡i sau báº±ng tiáº¿ng Viá»‡t, rÃºt ra chá»§ Ä‘á» chÃ­nh vÃ  nhá»¯ng thÃ´ng tin quan trá»ng:\n"
    for dialog in content:
        prompt += f"{user_name}: {dialog['query'].strip()}\n"
        prompt += f"{bot_name}: {dialog['response'].strip()}\n"
    prompt += "TÃ³m táº¯t:\n"
    return prompt

def summarize_overall_prompt(content):
    prompt = "HÃ£y tÃ³m táº¯t ngáº¯n gá»n nhá»¯ng sá»± kiá»‡n Ä‘Ã£ diá»…n ra dÆ°á»›i Ä‘Ã¢y, chá»‰ giá»¯ láº¡i cÃ¡c thÃ´ng tin quan trá»ng nháº¥t:\n"
    for date, summary_dict in content.items():
        # summary_text = summary_dict.get("content", "").strip()
        prompt += f"- NgÃ y {date}: {summary_dict}\n"
    prompt += "TÃ³m táº¯t tá»•ng quÃ¡t:\n"
    return prompt

def summarize_personality_prompt(content, user_name, bot_name):
    prompt = f"HÃ£y dá»±a vÃ o Ä‘oáº¡n há»™i thoáº¡i sau Ä‘á»ƒ phÃ¢n tÃ­ch tÃ­nh cÃ¡ch vÃ  cáº£m xÃºc cá»§a {user_name}, Ä‘á»“ng thá»i Ä‘á» xuáº¥t chiáº¿n lÆ°á»£c pháº£n há»“i phÃ¹ há»£p cho {bot_name}:\n"
    for dialog in content:
        prompt += f"{user_name}: {dialog['query'].strip()}\n"
        prompt += f"{bot_name}: {dialog['response'].strip()}\n"
    prompt += f"\nTÃ­nh cÃ¡ch, cáº£m xÃºc cá»§a {user_name} vÃ  chiáº¿n lÆ°á»£c pháº£n há»“i cá»§a {bot_name} lÃ :\n"
    return prompt

def summarize_overall_personality(content):
    prompt = "DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c phÃ¢n tÃ­ch vá» tÃ­nh cÃ¡ch vÃ  cáº£m xÃºc ngÆ°á»i dÃ¹ng trong nhiá»u Ä‘oáº¡n há»™i thoáº¡i:\n"
    for date, summary in content.items():
        prompt += f"- NgÃ y {date}: {summary}\n"
    prompt += "\nVui lÃ²ng tá»•ng há»£p thÃ nh má»™t báº£n tÃ³m táº¯t ngáº¯n gá»n vá» tÃ­nh cÃ¡ch tá»•ng thá»ƒ cá»§a ngÆ°á»i dÃ¹ng vÃ  cÃ¡ch pháº£n há»“i phÃ¹ há»£p nháº¥t tá»« AI:\n"
    return prompt



# TÃ³m táº¯t láº¡i cÃ¡c Ä‘oáº¡n Ä‘á»™i thoáº¡i cá»§a user_name tÆ°Æ¡ng á»©ng vÃ  lÆ°u vÃ o file db

def summarize_memory(memory_path, name, llm_client):
    bot_name = "AI"
    gen_prompt_num = 1
    with open(memory_path, 'r', encoding='utf8') as f:
        memory = json.load(f)

    for user_name, user_data in memory.items():
        if name is not None and user_name != name:
            continue

        print(f"Updating memory for user: {user_name}")

        history = user_data.get("history", {})
        if not history:
            continue

        user_data.setdefault("summary", {})
        user_data.setdefault("personality", {})

        for date, content in history.items():

            content_prompt = summarize_content_prompt(content, user_name, bot_name)
            personality_prompt = summarize_personality_prompt(content, user_name, bot_name)

            summary_text = llm_client.generate_text_simple(prompt=content_prompt, prompt_num=gen_prompt_num, language="vi")
            user_data["summary"][date] = {"content": ""}
            user_data["summary"][date]['content'] = summary_text

            personality_text = llm_client.generate_text_simple(prompt=personality_prompt, prompt_num=gen_prompt_num, language="vi")
            user_data["personality"][date] = personality_text

        overall_content_prompt = summarize_overall_prompt(user_data["summary"])
        overall_personality_prompt = summarize_overall_personality(user_data["personality"])
        user_data["overall_history"] = {'content': ''}
        user_data["overall_personality"] = {'content': ''}

        user_data["overall_history"]['content'] = llm_client.generate_text_simple(prompt=overall_content_prompt, prompt_num=gen_prompt_num, language="vi")
        user_data["overall_personality"]['content'] = llm_client.generate_text_simple(prompt=overall_personality_prompt, prompt_num=gen_prompt_num, language="vi")

    with open(memory_path, 'w', encoding='utf8') as f:
        json.dump(memory, f, ensure_ascii=False, indent=4)
        print(f"Memory updated for {'all users' if name is None else name}")

    return memory



# Class application Ä‘Æ°á»£c dÃ¹ng khi Ä‘Ã£ tÃ­ch há»£p Ä‘áº§y Ä‘á»§ cÃ¡c cÆ¡ cháº¿
class LLMClient:
    def __init__(self, user_name=None, model_path='../models/vinallama-7b-chat_q5_0.gguf'):
        self.user_name = user_name
        self.file_path_db = os.path.join(os.getcwd(), "data/test_json.json")
        self.llm = LLMs(model_path)

    def create_template(self, data: List[str], instruction: str):
        """
        Táº¡o prompt_template vá»›i:
            data: lÃ  1 list str bao gá»“m cÃ¡c dá»¯ liá»‡u nhÆ° thÃ´ng tin vá»  AI ghi nhá»› vÃ  dá»¯ liá»‡u muá»‘n AI tráº£ lá»i
            instruction: HÆ°á»›ng dáº«n chatbot tráº£ lá»i theo format mong muá»‘n
        """
        template = PromptTemplate(
            input_variables = data,
            template=instruction
        )
        return template

    def build_model_with_template(self, prompt_template):
        self.chatbot = self.llm.build_llm(prompt_template)
    # Need to update
    def summary_user(self):
        # Máº«u prompt Ä‘Æ¡n giáº£n dÃ¹ng cho tÃ³m táº¯t ná»™i dung
        default_prompt = PromptTemplate(
            input_variables=["text"],
            template=(
                "DÆ°á»›i Ä‘Ã¢y lÃ  Ä‘oáº¡n há»™i thoáº¡i giá»¯a má»™t con ngÆ°á»i vÃ  má»™t trá»£ lÃ½ AI thÃ´ng minh, am hiá»ƒu vá» tÃ¢m lÃ½ há»c.\n"
                "NgÆ°á»i dÃ¹ng: Xin chÃ o! HÃ£y giÃºp tÃ´i tÃ³m táº¯t ná»™i dung cá»§a Ä‘oáº¡n há»™i thoáº¡i.\n"
                "AI: Táº¥t nhiÃªn, tÃ´i sáº½ cá»‘ gáº¯ng háº¿t sá»©c Ä‘á»ƒ há»— trá»£ báº¡n.\n"
                "NgÆ°á»i dÃ¹ng: {text}\n"
            )
        )
        print('â–¶ï¸ Báº¯t Ä‘áº§u cáº­p nháº­t thÃ´ng tin ngÆ°á»i dÃ¹ng tá»« há»™i thoáº¡i...')
        tmp_model = self.llm.build_llm(default_prompt)
        client_simple = LLMClientSimple(tmp_model)
        summarize_memory(self.file_path_db, self.user_name, client_simple)


    def add_conservation_to_db(self, chat: dict):
        """
        ThÃªm má»™t Ä‘oáº¡n há»™i thoáº¡i vÃ o database JSON hiá»‡n cÃ³ (á»Ÿ self.file_path_db).
        Náº¿u file chÆ°a tá»“n táº¡i thÃ¬ táº¡o má»›i.
        Dá»¯ liá»‡u Ä‘áº§u vÃ o pháº£i theo format:
        {
            "2025-07-01": [  # ngÃ y
                {
                    "query": "...",
                    "response": "...",
                    "memory_strength": ...,
                    "last_recall_date": "...",
                    "memory_id": "..."
                }
            ]
        }
        """

        # Äá»c dá»¯ liá»‡u hiá»‡n cÃ³
        if os.path.exists(self.file_path_db):
            with open(self.file_path_db, 'r', encoding='utf-8') as f:
                try:
                    data = json.load(f)
                except json.JSONDecodeError:
                    data = {}
        else:
            data = {}

        # Khá»Ÿi táº¡o user náº¿u chÆ°a cÃ³
        if self.user_name not in data:
            data[self.user_name] = {
                "history": {},
                "summary": {},
                "personality": {},
                "overall_history": "",
                "overall_personality": ""
            }

        # ThÃªm chat vÃ o pháº§n history
        for date, messages in chat.items():

            if date not in data[self.user_name]["history"]:
                data[self.user_name]["history"][date] = []

            if isinstance(messages, dict):
                data[self.user_name]["history"][date].append(messages)

        # Ghi láº¡i vÃ o file
        with open(self.file_path_db, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

    def apply_forget(self):
      memory_loader = MemoryForgetLoader(self.file_path_db)
      memory_loader.update_forget_memory(self.user_name, datetime.datetime.now().strftime("%Y-%m-%d"))


    def generate_output(self, query: str, data, personal_info) -> str:
        """
        Sinh cÃ¢u tráº£ lá»i tá»« mÃ´ hÃ¬nh vá»›i cÃ¢u há»i `query` vÃ  ngá»¯ cáº£nh `context` (náº¿u cÃ³),
        sau Ä‘Ã³ lÆ°u Ä‘oáº¡n há»™i thoáº¡i vÃ o database JSON.
        """
        if not hasattr(self, "chatbot"):
            raise RuntimeError("Model chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o. HÃ£y gá»i build_model_with_template() trÆ°á»›c.")

        # Chuáº©n bá»‹ input cho LLMChain
        if personal_info == '':
            input_data = {
                        "data" : data,
                        "query" : query
                        }           
        else:
            input_data = {"personal_info" : personal_info,
                        "data" : data,
                        "query" : query
                        }

        # Sinh cÃ¢u tráº£ lá»i tá»« mÃ´ hÃ¬nh
        response = self.llm.generate_output(input_data)
        print("ğŸ” DEBUG RESPONSE RAW:", response)

        # Láº¥y ngÃ y hÃ´m nay dáº¡ng yyyy-mm-dd
        today = datetime.date.today().isoformat()

        # Táº¡o Ä‘oáº¡n há»™i thoáº¡i Ä‘á»ƒ lÆ°u
        new_entry = {
            "query": query,
            "response": response,
            "memory_strength": 1,  # cÃ³ thá»ƒ thay Ä‘á»•i theo logic AI
            "last_recall_date": today,
            "memory_id": f"{self.user_name}_{today}_{int(datetime.datetime.now().timestamp())}"
        }

        # ÄÃ³ng gÃ³i thÃ nh dict Ä‘á»ƒ lÆ°u vÃ o DB
        chat_to_save = {
                today: new_entry
        }

        # LÆ°u vÃ o JSON DB
        self.add_conservation_to_db(chat_to_save)

        return response






def clean_pdf_text(text: str) -> str:
    lines = text.splitlines()
    cleaned_lines = []

    for line in lines:
        # Loáº¡i bá» cÃ¡c dÃ²ng chá»©a sá»‘ trang hoáº·c header/footer láº·p láº¡i
        if re.search(r'Page \d+/\d+', line):
            continue
        if "TrÆ°á»ng Äáº¡i Há»c BÃ¡ch Khoa" in line:
            continue
        if "Khoa Khoa Há»c VÃ  KÄ© Thuáº­t MÃ¡y TÃ­nh" in line:
            continue
        if "Assignment Software Engineering" in line:
            continue
        cleaned_lines.append(line.strip())

    return "\n".join(cleaned_lines)
class GenerateData:
    def __init__(self, embedding_name: str, folder_data: str, faiss_save_path: str = "faiss_index"):
        self.model_embedding = HuggingFaceEmbeddings(model_name=embedding_name)

        self.folder_data = folder_data
        self.faiss_save_path = faiss_save_path
        self.documents = []
        self.vectorstore = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=512,
            chunk_overlap=100
        )
    def load_pdf_documents(self):
        """Äá»c má»—i file PDF thÃ nh 1 Document duy nháº¥t, chá»‰ lÆ°u metadata lÃ  tÃªn file"""
        for filename in os.listdir(self.folder_data):
            if filename.lower().endswith(".pdf"):
                filepath = os.path.join(self.folder_data, filename)
                try:
                    with open(filepath, "rb") as f:
                        reader = PyPDF2.PdfReader(f)
                        text = ""
                        for page in reader.pages:
                            text += page.extract_text() or ""
                    if text.strip():
                        metadata = {"source": filename}  # âœ… chá»‰ lÆ°u tÃªn file
                        self.documents.append(Document(page_content=text, metadata=metadata))
                except Exception as e:
                    print(f"âŒ Lá»—i Ä‘á»c {filename}: {e}")
        return self.documents

    def split_documents(self):
        """TÃ¡ch nhá» tÃ i liá»‡u thÃ nh cÃ¡c chunk Ä‘á»ƒ Ä‘Æ°a vÃ o FAISS"""
        if not self.documents:
            self.load_pdf_documents()
        return self.text_splitter.split_documents(self.documents)

    def build_faiss_index(self, save: bool = True):
        """Táº¡o FAISS index tá»« cÃ¡c document Ä‘Ã£ split"""
        docs = self.split_documents()
        self.vectorstore = FAISS.from_documents(docs, self.model_embedding)

        if save:
            self.vectorstore.save_local(self.faiss_save_path)
            print(f"âœ… FAISS index Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: {self.faiss_save_path}")
        return self.vectorstore

    def load_faiss_index(self):
        """Load FAISS index tá»« thÆ° má»¥c Ä‘Ã£ lÆ°u"""
        self.vectorstore = FAISS.load_local(
            self.faiss_save_path,
            self.model_embedding,
            allow_dangerous_deserialization=True
        )

        # âœ… GÃ¡n chunk_size vÃ o vectorstore
        self.vectorstore.chunk_size = 200  # hoáº·c truyá»n tá»« self náº¿u báº¡n muá»‘n linh hoáº¡t

        # âœ… GÃ¡n láº¡i method náº¿u Ä‘ang dÃ¹ng FAISS custom
        self.vectorstore.similarity_search_with_score_by_vector = MethodType(
            similarity_search_with_score_by_vector,
            self.vectorstore
        )

        print(f"âœ… FAISS index Ä‘Ã£ Ä‘Æ°á»£c load tá»«: {self.faiss_save_path}")
        return self.vectorstore

    def query(self, question: str, k: int = 3):
        """Truy váº¥n cÃ¢u há»i vÃ o FAISS vÃ  tráº£ vá» top-k cÃ¢u tráº£ lá»i"""
        if not self.vectorstore:
            self.load_faiss_index()
        if not hasattr(self.vectorstore, "chunk_size"):
            self.vectorstore.chunk_size = 200
        results = self.vectorstore.similarity_search(question, k=k)
        return [clean_pdf_text(x.page_content) for x in results]