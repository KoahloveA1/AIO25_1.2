{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wqhasBqXoCYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb8840e-bb62-4ec1-d386-c4028e069fc1"
      },
      "id": "wqhasBqXoCYG",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "drive_cache = \"/content/drive/MyDrive/llama_cpp_build/llama_cpp\"\n",
        "site_pkg = \"/usr/local/lib/python3.11/dist-packages/llama_cpp\"\n",
        "\n",
        "\n",
        "if os.path.exists(drive_cache):\n",
        "    print(\"Đang copy llama_cpp từ Google Drive...\")\n",
        "    shutil.copytree(drive_cache, site_pkg, dirs_exist_ok=True)\n",
        "else:\n",
        "    print(\"Chưa có cache → cài từ pip + CUDA\")\n",
        "    !CMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
        "    shutil.copytree(site_pkg, drive_cache, dirs_exist_ok=True)\n"
      ],
      "metadata": {
        "id": "OcNof5BbJlGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eadfe12-0c98-48a3-ad90-66359057287f"
      },
      "id": "OcNof5BbJlGN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đang copy llama_cpp từ Google Drive...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirement.txt"
      ],
      "metadata": {
        "id": "dXQamG7Plq4B"
      },
      "id": "dXQamG7Plq4B",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "CACHE_DIR = \"/content/drive/MyDrive/model_cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# Cấu hình các biến môi trường để cache transformers & llama.cpp\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = CACHE_DIR\n",
        "os.environ[\"HF_HOME\"] = CACHE_DIR\n",
        "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = CACHE_DIR\n",
        "os.environ[\"LLAMA_CPP_CACHE_DIR\"] = CACHE_DIR\n"
      ],
      "metadata": {
        "id": "yembEOseoQ1C"
      },
      "id": "yembEOseoQ1C",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = \"vinallama-7b-chat_q5_0.gguf\"\n",
        "model_path = os.path.join('/content/drive/MyDrive/model_cache/models--vilm--vinallama-7b-chat-GGUF/snapshots/5c76606edd7f6c714fba2988990dedecba33c0ff/', model_filename)\n",
        "\n",
        "if not os.path.isfile(model_path):\n",
        "    print(\"Đang tải mô hình Q5_0…\")\n",
        "    !pip install -q huggingface_hub\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    hf_hub_download(\n",
        "        repo_id=\"vilm/vinallama-7b-chat-GGUF\",\n",
        "        filename=model_filename,\n",
        "        cache_dir=CACHE_DIR,\n",
        "        library_name=\"llama.cpp\",\n",
        "        library_version=\"latest\"\n",
        "    )\n",
        "else:\n",
        "    print(\"Mô hình đã tồn tại, không tải lại.\")\n"
      ],
      "metadata": {
        "id": "-NKV971xoWzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2da0a5-0eb9-457e-8232-b2862c815bce"
      },
      "id": "-NKV971xoWzJ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mô hình đã tồn tại, không tải lại.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "53e4f5e7",
      "metadata": {
        "id": "53e4f5e7"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import copy\n",
        "import json\n",
        "\n",
        "\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "import datetime\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ba696d96",
      "metadata": {
        "id": "ba696d96"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/model_cache/models--vilm--vinallama-7b-chat-GGUF/snapshots/5c76606edd7f6c714fba2988990dedecba33c0ff/vinallama-7b-chat_q5_0.gguf'\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Giữ lại cấu trúc dạng đoạn văn hoặc liệt kê.\n",
        "    Xóa khoảng trắng đầu dòng, dòng trống, và chuẩn hóa dòng liên tục.\n",
        "    \"\"\"\n",
        "    # Tách từng dòng, loại bỏ khoảng trắng đầu/cuối từng dòng\n",
        "    lines = [line.strip() for line in text.strip().splitlines()]\n",
        "\n",
        "    # Loại bỏ dòng trống\n",
        "    lines = [line for line in lines if line]\n",
        "\n",
        "    # Nếu toàn bộ chỉ là một đoạn văn không xuống dòng có chủ ý, nối lại một dòng\n",
        "    if all(not re.match(r'^[-•*]|^[A-Z][a-z]+:', line) for line in lines):\n",
        "        return ' '.join(lines)\n",
        "\n",
        "    # Ngược lại: giữ xuống dòng giữa các đoạn có ý nghĩa\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "# Class khởi tạo model llms\n",
        "class LLMs:\n",
        "    def __init__(self, model_id):\n",
        "        self.__model = LlamaCpp(\n",
        "            model_path=model_id,\n",
        "            n_gpu_layers=-1,\n",
        "            n_ctx=2048,\n",
        "            temperature=0.1,\n",
        "            max_tokens=256,\n",
        "            verbose=True,\n",
        "            n_batch=512,\n",
        "            n_threads=8,\n",
        "            use_mmap=True,\n",
        "            use_mlock=True,\n",
        "            seed=-1,\n",
        "            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
        "        )\n",
        "\n",
        "    def build_llm(self, prompt_template: PromptTemplate):\n",
        "        self.__llm_chain = LLMChain(llm=self.__model, prompt=prompt_template)\n",
        "        return self.__llm_chain\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.__model\n",
        "\n",
        "    def generate_output(self, input_dict: dict):\n",
        "        if not hasattr(self, '_LLMs__llm_chain'):\n",
        "            raise RuntimeError(\"LLM chain not built. Call build_llm() first.\")\n",
        "        raw_output = self.__llm_chain.run(input_dict)\n",
        "        return clean_text(raw_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "121d9d9f",
      "metadata": {
        "id": "121d9d9f"
      },
      "outputs": [],
      "source": [
        "def forgetting_curve(t, S):\n",
        "    \"\"\"\n",
        "    Calculate the retention of information at time t based on the forgetting curve.\n",
        "\n",
        "    :param t: Time elapsed since the information was learned (in days).\n",
        "    :type t: float\n",
        "    :param S: Strength of the memory.\n",
        "    :type S: float\n",
        "    :return: Retention of information at time t.\n",
        "    :rtype: float\n",
        "    Memory strength is a concept used in memory models to represent the durability or stability of a memory trace in the brain.\n",
        "    In the context of the forgetting curve, memory strength (denoted as 'S') is a parameter that\n",
        "    influences the rate at which information is forgotten.\n",
        "    The higher the memory strength, the slower the rate of forgetting,\n",
        "    and the longer the information is retained.\n",
        "    \"\"\"\n",
        "    return math.exp(-t / 5*S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6ba192b3",
      "metadata": {
        "id": "6ba192b3"
      },
      "outputs": [],
      "source": [
        "def get_docs_with_score(docs_with_score):\n",
        "    docs=[]\n",
        "    for doc, score in docs_with_score:\n",
        "        doc.metadata[\"score\"] = score\n",
        "        docs.append(doc)\n",
        "    return docs\n",
        "\n",
        "\n",
        "def seperate_list(ls: List[int]) -> List[List[int]]:\n",
        "    lists = []\n",
        "    ls1 = [ls[0]]\n",
        "    for i in range(1, len(ls)):\n",
        "        if ls[i-1] + 1 == ls[i]:\n",
        "            ls1.append(ls[i])\n",
        "        else:\n",
        "            lists.append(ls1)\n",
        "            ls1 = [ls[i]]\n",
        "    lists.append(ls1)\n",
        "    return lists\n",
        "\n",
        "\n",
        "def similarity_search_with_score_by_vector(\n",
        "        self,\n",
        "        embedding: List[float],\n",
        "        k: int = 4,\n",
        "        **kwargs\n",
        "    ) -> List[Tuple[Document, float]]:\n",
        "        scores, indices = self.index.search(np.array([embedding], dtype=np.float32), k)\n",
        "        docs = []\n",
        "        id_set = set()\n",
        "        for j, i in enumerate(indices[0]):\n",
        "            if i == -1:\n",
        "                # This happens when not enough docs are returned.\n",
        "                continue\n",
        "            _id = self.index_to_docstore_id[i]\n",
        "            doc = self.docstore.search(_id)\n",
        "            id_set.add(i)\n",
        "            docs_len = len(doc.page_content)\n",
        "            for k in range(1, max(i, len(self.index_to_docstore_id)-i)):\n",
        "                for l in [i+k, i-k]:\n",
        "                    if 0 <= l < len(self.index_to_docstore_id):\n",
        "                        _id0 = self.index_to_docstore_id[l]\n",
        "                        doc0 = self.docstore.search(_id0)\n",
        "                        # print(doc0.metadata)\n",
        "                        # exit()\n",
        "                        if docs_len + len(doc0.page_content) > self.chunk_size:\n",
        "                            break\n",
        "                        # print(doc0)\n",
        "                        elif doc0.metadata[\"source\"] == doc.metadata[\"source\"]:\n",
        "                            docs_len += len(doc0.page_content)\n",
        "                            id_set.add(l)\n",
        "        id_list = sorted(list(id_set))\n",
        "        id_lists = seperate_list(id_list)\n",
        "        for id_seq in id_lists:\n",
        "            for id in id_seq:\n",
        "                if id == id_seq[0]:\n",
        "                    _id = self.index_to_docstore_id[id]\n",
        "                    doc = self.docstore.search(_id)\n",
        "                else:\n",
        "                    _id0 = self.index_to_docstore_id[id]\n",
        "                    doc0 = self.docstore.search(_id0)\n",
        "                    doc.page_content += doc0.page_content\n",
        "            if not isinstance(doc, Document):\n",
        "                raise ValueError(f\"Could not find document for id {_id}, got {doc}\")\n",
        "            docs.append((doc, scores[0][j]))\n",
        "        return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2119a997",
      "metadata": {
        "id": "2119a997"
      },
      "outputs": [],
      "source": [
        "class MemoryForgetLoader:\n",
        "    # Khởi tạo một dict tạm để lưu trũ các thông tin.\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self.memory_bank = {}\n",
        "\n",
        "    def _get_date_difference(self, date1: str, date2: str) -> int:\n",
        "        date_format = \"%Y-%m-%d\"\n",
        "        d1 = datetime.datetime.strptime(date1, date_format)\n",
        "        d2 = datetime.datetime.strptime(date2, date_format)\n",
        "        return (d2 - d1).days\n",
        "\n",
        "\n",
        "    def write_memories(self, out_file):\n",
        "        with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            print(f'Successfully write to {out_file}')\n",
        "            json.dump(self.memory_bank, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    def load_memories(self, memory_file):\n",
        "        # print(memory_file)\n",
        "        with open(memory_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.memory_bank = json.load(f)\n",
        "    # Hàm khởi tạo và cập nhật metadata vào db có kèm theo cơ chế  suy giảm trí nhớ.\n",
        "    def update_forget_memory(self, name, cur_date):\n",
        "        with open(self.file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        docs_about_user = []\n",
        "        for user_name, info in data.items():\n",
        "            self.memory_bank[user_name] = copy.deepcopy(info)\n",
        "            if user_name != name:\n",
        "                continue\n",
        "            if 'history' not in info.keys():\n",
        "                continue\n",
        "            for date, dialog in info['history'].items():\n",
        "                # Khởi tạo mảng các đoạn đối thoại sẽ bị quên của user trong date\n",
        "                forget_index = []\n",
        "                tmp_str = f\"Đây là đoạn đối thoại vào {date} \"\n",
        "                for i, chat in enumerate(dialog):\n",
        "\n",
        "                    # Khởi tạo metadata hoặc lấy metadata để trích xuất quên của chabot về đoạn hội thoại nào đó\n",
        "                    memory_strength = chat.get('memory_strength', 1)\n",
        "                    last_recall_date = chat.get('last_recall_date', date)\n",
        "                    memory_id = chat.get('memory_id', f'{user_name}_{date}_{i}')\n",
        "                    query = f\"[|User|]: {chat['query']}\"\n",
        "                    response = f\"[|AI|]: {chat['response']}\"\n",
        "                    tmp_str += query + response\n",
        "                    metadata = {\n",
        "                        'memory_strength': memory_strength,\n",
        "                        'last_recall_date': last_recall_date,\n",
        "                        'memory_id': memory_id\n",
        "                    }\n",
        "                    self.memory_bank[user_name]['history'][date][i].update(metadata)\n",
        "\n",
        "                    diff_date = self._get_date_difference(last_recall_date, cur_date)\n",
        "                    forget_probability = forgetting_curve(diff_date, memory_strength)\n",
        "                    # Thử khả năng liệu có quên hay không?\n",
        "                    if random.random() > forget_probability:\n",
        "                        forget_index.append(i)\n",
        "                    else:\n",
        "                        # Lưu thông tin người dùng để tạo documents với metadata để dễ retrival\n",
        "                        docs_about_user.append(Document(page_content=tmp_str,metadata=metadata))\n",
        "                if len(forget_index) > 0:\n",
        "                    forget_index.sort(reverse=True)\n",
        "                    for idd in forget_index:\n",
        "                        self.memory_bank[user_name]['history'][date].pop(idd)\n",
        "                        print(f'Delete convestion of {user_name} on {date}')\n",
        "                if len(self.memory_bank[user_name]['history'][date]) == 0:\n",
        "                    self.memory_bank[user_name]['history'].pop(date)\n",
        "                    self.memory_bank[user_name]['summary'].pop(date)\n",
        "\n",
        "                if 'summary' in info.keys():\n",
        "                    if date in self.memory_bank[user_name]['summary'].keys():\n",
        "                        summary = f\"Đây là tóm tắt vào ngày {date}\"\n",
        "                        summary += data[user_name]['summary'][date]['content']\n",
        "                        memory_strength = self.memory_bank[user_name]['summary'][date].get('memory_strength',1)\n",
        "                        last_recall_date = self.memory_bank[user_name][\"summary\"][date].get('last_recall_date',date)\n",
        "                        metadata = {\n",
        "                            'memory_strength':memory_strength,\n",
        "                            'memory_id':f'{user_name}_{date}_summary',\n",
        "                            'last_recall_date':last_recall_date,\"source\":f'{user_name}_{date}_summary'\n",
        "                        }\n",
        "                        self.memory_bank[user_name]['summary'][date].update(metadata)\n",
        "                        docs_about_user.append(Document(page_content=summary,metadata=metadata))\n",
        "                # if 'overall_history' in info.keys():\n",
        "                #     metadata = {\n",
        "                #         'overall_history' : user_name\n",
        "                #     }\n",
        "                #     docs_about_user.append(Document(page_content=data[user_name]['overall_history'], metadata=metadata))\n",
        "                # if 'overall_personality' in info.keys():\n",
        "                #     metadata = {\n",
        "                #         'overall_personality' : user_name\n",
        "                #     }\n",
        "                #     docs_about_user.append(Document(page_content=data[user_name]['overall_personality'], metadata=metadata))\n",
        "        self.write_memories(self.file_path)\n",
        "        return docs_about_user\n",
        "\n",
        "    def update_memory_when_searched(self, recalled_memos,user,cur_date):\n",
        "        for recalled in recalled_memos:\n",
        "            recalled_id = recalled.metadata['memory_id']\n",
        "            recalled_date = recalled_id.split('_')[1]\n",
        "            for i,memory in enumerate(self.memory_bank[user]['history'][recalled_date]):\n",
        "                if memory['memory_id'] == recalled_id:\n",
        "                    self.memory_bank[user]['history'][recalled_date][i]['memory_strength'] += 1\n",
        "                    self.memory_bank[user]['history'][recalled_date][i]['last_recall_date'] = cur_date\n",
        "                    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a1387ecc",
      "metadata": {
        "id": "a1387ecc"
      },
      "outputs": [],
      "source": [
        "# Class khởi tạo một instance có khả năng truy xuất bộ nhớ.\n",
        "from types import MethodType\n",
        "class MemoryRetrival:\n",
        "    def __init__(self, embedding_name, top_k, chunk_size, user, file_path):\n",
        "        self.embedding_model = HuggingFaceEmbeddings(model_name=embedding_name)\n",
        "        self.top_k = top_k\n",
        "        self.chunk_size = chunk_size\n",
        "        self.user = user\n",
        "        self.memory_path = file_path\n",
        "\n",
        "    def helper_load_file(self, file_name: str, user_name: str, cur_date):\n",
        "        loader = MemoryForgetLoader(file_name)\n",
        "        docs = loader.update_forget_memory(user_name,cur_date)\n",
        "        splitter = RecursiveCharacterTextSplitter()\n",
        "        docs = splitter.split_documents(docs)\n",
        "        return docs, loader\n",
        "\n",
        "    def init_memory_index(self, file_name: str, saving_path: str, user_name: str, cur_date: str):\n",
        "        # Load và chunk văn bản\n",
        "        docs, self.memory_loader = self.helper_load_file(file_name, user_name, cur_date)\n",
        "\n",
        "        if not docs:\n",
        "            print(\"❌ Không có tài liệu nào được tạo.\")\n",
        "            return\n",
        "\n",
        "        # Tạo FAISS index từ tài liệu\n",
        "        vector_store = FAISS.from_documents(docs, self.embedding_model)\n",
        "\n",
        "        # Đảm bảo thư mục tồn tại\n",
        "        os.makedirs(saving_path, exist_ok=True)\n",
        "\n",
        "        # Đặt tên file FAISS theo user_name\n",
        "        file_id = user_name or f\"memory_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        full_path = os.path.join(saving_path, file_id)\n",
        "\n",
        "        # Lưu FAISS\n",
        "        vector_store.save_local(full_path)\n",
        "        print(f\"📦 FAISS index đã được lưu tại: {full_path}\")\n",
        "\n",
        "\n",
        "    def load_memory_index(self, vs_path: str):\n",
        "        \"\"\"Tải FAISS index đã lưu từ vs_path\"\"\"\n",
        "        vector_store = FAISS.load_local(\n",
        "            vs_path,\n",
        "            self.embedding_model,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "\n",
        "        # Gán hàm tìm kiếm custom nếu có (tối ưu ghép chunk)\n",
        "        FAISS.similarity_search_with_score_by_vector = MethodType(similarity_search_with_score_by_vector, vector_store)\n",
        "\n",
        "        # Gán chunk_size cho vector_store để sử dụng trong search nếu cần\n",
        "        vector_store.chunk_size = self.chunk_size\n",
        "\n",
        "        print(f\"✅ Đã load memory index từ: {vs_path}\")\n",
        "        return vector_store\n",
        "\n",
        "\n",
        "    def search_memory(self, query: str, vector_store, cur_date=''):\n",
        "        related_docs_with_score = vector_store.similarity_search_with_score(query, k=self.top_k)\n",
        "        related_docs = get_docs_with_score(related_docs_with_score)\n",
        "\n",
        "        # ✅ Tách ngày từ memory_id để sắp theo ngày + memory_id\n",
        "        def extract_date(doc):\n",
        "            memory_id = doc.metadata[\"memory_id\"]\n",
        "            return memory_id.split(\"_\")[1]  # ví dụ: \"2025-06-25\"\n",
        "\n",
        "        related_docs = sorted(\n",
        "            related_docs,\n",
        "            key=lambda x: (extract_date(x), x.metadata[\"memory_id\"])\n",
        "        )\n",
        "\n",
        "        pre_date = ''\n",
        "        date_docs = []\n",
        "        dates = []\n",
        "        cur_date = cur_date if cur_date else datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        for doc in related_docs:\n",
        "            date_str = extract_date(doc)\n",
        "            doc.page_content = doc.page_content.replace(f'Đây là đoạn đối thoại vào {date_str}：', '').strip()\n",
        "\n",
        "            if date_str != pre_date:\n",
        "                date_docs.append(doc.page_content)\n",
        "                pre_date = date_str\n",
        "                dates.append(pre_date)\n",
        "            else:\n",
        "                date_docs[-1] += f'\\n{doc.page_content}'\n",
        "\n",
        "        self.memory_loader.update_memory_when_searched(related_docs, user=self.user, cur_date=cur_date)\n",
        "        self.save_updated_memory()\n",
        "        return date_docs, ', '.join(dates)\n",
        "\n",
        "    def save_updated_memory(self):\n",
        "        self.memory_loader.write_memories(self.memory_path)#.replace('.json','_forget_format.json'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4e8d2d10",
      "metadata": {
        "id": "4e8d2d10"
      },
      "outputs": [],
      "source": [
        "class LLMClientSimple:\n",
        "    def __init__(self, llm_chain):\n",
        "        self.llm_chain = llm_chain  # LLMChain từ mô hình local đã được build\n",
        "\n",
        "    def generate_text_simple(self, prompt, prompt_num=1, language='vi'):\n",
        "        \"\"\"\n",
        "        Sinh văn bản từ local model với prompt đầu vào.\n",
        "        prompt_num không áp dụng với local LLM, chỉ giữ lại cho tương thích.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = self.llm_chain.run({\"text\": prompt})\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"Lỗi khi sinh văn bản từ local model: {e}\")\n",
        "            return \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9eb8e870",
      "metadata": {
        "id": "9eb8e870"
      },
      "outputs": [],
      "source": [
        "# Tạo prompt cho tóm tắt\n",
        "\n",
        "def summarize_content_prompt(content, user_name, bot_name):\n",
        "    prompt = \"Hãy tóm tắt nội dung cuộc hội thoại sau bằng tiếng Việt, rút ra chủ đề chính và những thông tin quan trọng:\\n\"\n",
        "    for dialog in content:\n",
        "        prompt += f\"{user_name}: {dialog['query'].strip()}\\n\"\n",
        "        prompt += f\"{bot_name}: {dialog['response'].strip()}\\n\"\n",
        "    prompt += \"Tóm tắt:\\n\"\n",
        "    return prompt\n",
        "\n",
        "def summarize_overall_prompt(content):\n",
        "    prompt = \"Hãy tóm tắt ngắn gọn những sự kiện đã diễn ra dưới đây, chỉ giữ lại các thông tin quan trọng nhất:\\n\"\n",
        "    for date, summary_dict in content:\n",
        "        summary = summary_dict['content']\n",
        "        prompt += f\"- Ngày {date}: {summary.strip()}\\n\"\n",
        "    prompt += \"Tóm tắt tổng quát:\\n\"\n",
        "    return prompt\n",
        "\n",
        "def summarize_personality_prompt(content, user_name, bot_name):\n",
        "    prompt = f\"Hãy dựa vào đoạn hội thoại sau để phân tích tính cách và cảm xúc của {user_name}, đồng thời đề xuất chiến lược phản hồi phù hợp cho {bot_name}:\\n\"\n",
        "    for dialog in content:\n",
        "        prompt += f\"{user_name}: {dialog['query'].strip()}\\n\"\n",
        "        prompt += f\"{bot_name}: {dialog['response'].strip()}\\n\"\n",
        "    prompt += f\"\\nTính cách, cảm xúc của {user_name} và chiến lược phản hồi của {bot_name} là:\\n\"\n",
        "    return prompt\n",
        "\n",
        "def summarize_overall_personality(content):\n",
        "    prompt = \"Dưới đây là các phân tích về tính cách và cảm xúc người dùng trong nhiều đoạn hội thoại:\\n\"\n",
        "    for date, summary in content:\n",
        "        prompt += f\"- Ngày {date}: {summary.strip()}\\n\"\n",
        "    prompt += \"\\nVui lòng tổng hợp thành một bản tóm tắt ngắn gọn về tính cách tổng thể của người dùng và cách phản hồi phù hợp nhất từ AI:\\n\"\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7f3bdb02",
      "metadata": {
        "id": "7f3bdb02"
      },
      "outputs": [],
      "source": [
        "# Tóm tắt lại các đoạn đội thoại của user_name tương ứng và lưu vào file db\n",
        "\n",
        "def summarize_memory(memory_path, name, llm_client):\n",
        "    bot_name = \"AI\"\n",
        "    gen_prompt_num = 1\n",
        "    with open(memory_path, 'r', encoding='utf8') as f:\n",
        "        memory = json.load(f)\n",
        "\n",
        "    for user_name, user_data in memory.items():\n",
        "        if name is not None and user_name != name:\n",
        "            continue\n",
        "\n",
        "        print(f\"Updating memory for user: {user_name}\")\n",
        "\n",
        "        history = user_data.get(\"history\", {})\n",
        "        if not history:\n",
        "            continue\n",
        "\n",
        "        user_data.setdefault(\"summary\", {})\n",
        "        user_data.setdefault(\"personality\", {})\n",
        "\n",
        "        for date, content in history.items():\n",
        "            summary_exists = bool(user_data[\"summary\"].get(date))\n",
        "            personality_exists = bool(user_data[\"personality\"].get(date))\n",
        "\n",
        "            content_prompt = summarize_content_prompt(content, user_name, bot_name)\n",
        "            personality_prompt = summarize_personality_prompt(content, user_name, bot_name)\n",
        "\n",
        "            if not summary_exists:\n",
        "                summary_text = llm_client.generate_text_simple(prompt=content_prompt, prompt_num=gen_prompt_num, language=\"vi\")\n",
        "                user_data[\"summary\"][date] = {\"content\": summary_text}\n",
        "\n",
        "            if not personality_exists:\n",
        "                personality_text = llm_client.generate_text_simple(prompt=personality_prompt, prompt_num=gen_prompt_num, language=\"vi\")\n",
        "                user_data[\"personality\"][date] = personality_text\n",
        "\n",
        "        overall_content_prompt = summarize_overall_prompt(list(user_data[\"summary\"].items()))\n",
        "        overall_personality_prompt = summarize_overall_personality(list(user_data[\"personality\"].items()))\n",
        "\n",
        "        user_data[\"overall_history\"] = llm_client.generate_text_simple(prompt=overall_content_prompt, prompt_num=gen_prompt_num, language=\"vi\")\n",
        "        user_data[\"overall_personality\"] = llm_client.generate_text_simple(prompt=overall_personality_prompt, prompt_num=gen_prompt_num, language=\"vi\")\n",
        "\n",
        "    with open(memory_path, 'w', encoding='utf8') as f:\n",
        "        json.dump(memory, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"Memory updated for {'all users' if name is None else name}\")\n",
        "\n",
        "    return memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "11b8ef87",
      "metadata": {
        "id": "11b8ef87"
      },
      "outputs": [],
      "source": [
        "# Class application được dùng khi đã tích hợp đầy đủ các cơ chế\n",
        "class LLMClient:\n",
        "    def __init__(self, user_name=None, model_path='../models/vinallama-7b-chat_q5_0.gguf'):\n",
        "        self.user_name = user_name\n",
        "        self.file_path_db = os.path.join(os.getcwd(), \"data/test_json.json\")\n",
        "        self.llm = LLMs(model_path)\n",
        "\n",
        "    def create_template(self, data: List[str], instruction: str):\n",
        "        \"\"\"\n",
        "        Tạo prompt_template với:\n",
        "            data: là 1 list str bao gồm các dữ liệu như thông tin về  AI ghi nhớ và dữ liệu muốn AI trả lời\n",
        "            instruction: Hướng dẫn chatbot trả lời theo format mong muốn\n",
        "        \"\"\"\n",
        "        template = PromptTemplate(\n",
        "            input_variables = data,\n",
        "            template=instruction\n",
        "        )\n",
        "        return template\n",
        "\n",
        "    def build_model_with_template(self, prompt_template):\n",
        "        self.chatbot = self.llm.build_llm(prompt_template)\n",
        "    # Need to update\n",
        "    def summary_user(self):\n",
        "        default_prompt = PromptTemplate(\n",
        "        input_variables=[\"text\"],\n",
        "          template=\"\"\"\n",
        "      Bạn là một trợ lý AI lịch sự, rõ ràng. Vui lòng không sử dụng ký tự xuống dòng, ký tự đặc biệt như \"\\\\n\", \"\\\\t\" hay định dạng Markdown. Trả lời trực tiếp và ngắn gọn, trên một dòng duy nhất.\n",
        "\n",
        "      Câu hỏi: {text}\n",
        "      Trả lời:\n",
        "      \"\"\"\n",
        "      )\n",
        "        print('Cập nhật thông tin user')\n",
        "        tmp_model = self.llm.build_llm(default_prompt)\n",
        "        client_simple = LLMClientSimple(tmp_model)\n",
        "        summarize_memory(self.file_path_db, self.user_name, client_simple)\n",
        "\n",
        "\n",
        "    def add_conservation_to_db(self, chat: dict):\n",
        "        \"\"\"\n",
        "        Thêm một đoạn hội thoại vào database JSON hiện có (ở self.file_path_db).\n",
        "        Nếu file chưa tồn tại thì tạo mới.\n",
        "        Dữ liệu đầu vào phải theo format:\n",
        "        {\n",
        "            \"2025-07-01\": [  # ngày\n",
        "                {\n",
        "                    \"query\": \"...\",\n",
        "                    \"response\": \"...\",\n",
        "                    \"memory_strength\": ...,\n",
        "                    \"last_recall_date\": \"...\",\n",
        "                    \"memory_id\": \"...\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        # Đọc dữ liệu hiện có\n",
        "        if os.path.exists(self.file_path_db):\n",
        "            with open(self.file_path_db, 'r', encoding='utf-8') as f:\n",
        "                try:\n",
        "                    data = json.load(f)\n",
        "                except json.JSONDecodeError:\n",
        "                    data = {}\n",
        "        else:\n",
        "            data = {}\n",
        "\n",
        "        # Khởi tạo user nếu chưa có\n",
        "        if self.user_name not in data:\n",
        "            data[self.user_name] = {\n",
        "                \"history\": {},\n",
        "                \"summary\": {},\n",
        "                \"personality\": {},\n",
        "                \"overall_history\": \"\",\n",
        "                \"overall_personality\": \"\"\n",
        "            }\n",
        "\n",
        "        # Thêm chat vào phần history\n",
        "        for date, messages in chat.items():\n",
        "            if \"history\" not in data[self.user_name]:\n",
        "                data[self.user_name][\"history\"] = {}\n",
        "\n",
        "            if date not in data[self.user_name][\"history\"]:\n",
        "                data[self.user_name][\"history\"][date] = []\n",
        "\n",
        "            if isinstance(messages, list):\n",
        "                data[self.user_name][\"history\"][date].extend(messages)\n",
        "\n",
        "        # Ghi lại vào file\n",
        "        with open(self.file_path_db, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    def apply_forget(self):\n",
        "      memory_loader = MemoryForgetLoader(self.file_path_db)\n",
        "      memory_loader.update_forget_memory(self.user_name, datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "\n",
        "    def generate_output(self, query: str, data, personal_info) -> str:\n",
        "        \"\"\"\n",
        "        Sinh câu trả lời từ mô hình với câu hỏi `query` và ngữ cảnh `context` (nếu có),\n",
        "        sau đó lưu đoạn hội thoại vào database JSON.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"chatbot\"):\n",
        "            raise RuntimeError(\"Model chưa được khởi tạo. Hãy gọi build_model_with_template() trước.\")\n",
        "\n",
        "        # Chuẩn bị input cho LLMChain\n",
        "        input_data = {\"personal_info\" : personal_info,\n",
        "                      \"data\" : data,\n",
        "                      \"query\" : query\n",
        "                      }\n",
        "\n",
        "        # Sinh câu trả lời từ mô hình\n",
        "        response = self.llm.generate_output(input_data)\n",
        "\n",
        "        # Lấy ngày hôm nay dạng yyyy-mm-dd\n",
        "        today = datetime.date.today().isoformat()\n",
        "\n",
        "        # Tạo đoạn hội thoại để lưu\n",
        "        new_entry = {\n",
        "            \"query\": query,\n",
        "            \"response\": response,\n",
        "            \"memory_strength\": 1,  # có thể thay đổi theo logic AI\n",
        "            \"last_recall_date\": today,\n",
        "            \"memory_id\": f\"{self.user_name}_{today}_{int(datetime.datetime.now().timestamp())}\"\n",
        "        }\n",
        "\n",
        "        # Đóng gói thành dict để lưu vào DB\n",
        "        chat_to_save = {\n",
        "            \"history\": {\n",
        "                today: [new_entry]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Lưu vào JSON DB\n",
        "        self.add_conservation_to_db(chat_to_save)\n",
        "\n",
        "        return response\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "86406df3",
      "metadata": {
        "id": "86406df3"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "def clean_pdf_text(text: str) -> str:\n",
        "    lines = text.splitlines()\n",
        "    cleaned_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        # Loại bỏ các dòng chứa số trang hoặc header/footer lặp lại\n",
        "        if re.search(r'Page \\d+/\\d+', line):\n",
        "            continue\n",
        "        if \"Trường Đại Học Bách Khoa\" in line:\n",
        "            continue\n",
        "        if \"Khoa Khoa Học Và Kĩ Thuật Máy Tính\" in line:\n",
        "            continue\n",
        "        if \"Assignment Software Engineering\" in line:\n",
        "            continue\n",
        "        cleaned_lines.append(line.strip())\n",
        "\n",
        "    return \"\\n\".join(cleaned_lines)\n",
        "class GenerateData:\n",
        "    def __init__(self, embedding_name: str, folder_data: str, faiss_save_path: str = \"./faiss_index\"):\n",
        "        self.model_embedding = HuggingFaceEmbeddings(model_name=embedding_name)\n",
        "        self.folder_data = folder_data\n",
        "        self.faiss_save_path = faiss_save_path\n",
        "        self.documents = []\n",
        "        self.vectorstore = None\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=512,\n",
        "            chunk_overlap=100\n",
        "        )\n",
        "    def load_pdf_documents(self):\n",
        "        \"\"\"Đọc mỗi file PDF thành 1 Document duy nhất, chỉ lưu metadata là tên file\"\"\"\n",
        "        for filename in os.listdir(self.folder_data):\n",
        "            if filename.lower().endswith(\".pdf\"):\n",
        "                filepath = os.path.join(self.folder_data, filename)\n",
        "                try:\n",
        "                    with open(filepath, \"rb\") as f:\n",
        "                        reader = PyPDF2.PdfReader(f)\n",
        "                        text = \"\"\n",
        "                        for page in reader.pages:\n",
        "                            text += page.extract_text() or \"\"\n",
        "                    if text.strip():\n",
        "                        metadata = {\"source\": filename}  # ✅ chỉ lưu tên file\n",
        "                        self.documents.append(Document(page_content=text, metadata=metadata))\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Lỗi đọc {filename}: {e}\")\n",
        "        return self.documents\n",
        "\n",
        "    def split_documents(self):\n",
        "        \"\"\"Tách nhỏ tài liệu thành các chunk để đưa vào FAISS\"\"\"\n",
        "        if not self.documents:\n",
        "            self.load_pdf_documents()\n",
        "        return self.text_splitter.split_documents(self.documents)\n",
        "\n",
        "    def build_faiss_index(self, save: bool = True):\n",
        "        \"\"\"Tạo FAISS index từ các document đã split\"\"\"\n",
        "        docs = self.split_documents()\n",
        "        self.vectorstore = FAISS.from_documents(docs, self.model_embedding)\n",
        "\n",
        "        if save:\n",
        "            self.vectorstore.save_local(self.faiss_save_path)\n",
        "            print(f\"✅ FAISS index đã được lưu tại: {self.faiss_save_path}\")\n",
        "        return self.vectorstore\n",
        "\n",
        "    def load_faiss_index(self):\n",
        "        \"\"\"Load FAISS index từ thư mục đã lưu\"\"\"\n",
        "        self.vectorstore = FAISS.load_local(\n",
        "            self.faiss_save_path,\n",
        "            self.model_embedding,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "\n",
        "        # ✅ Gán chunk_size vào vectorstore\n",
        "        self.vectorstore.chunk_size = 200  # hoặc truyền từ self nếu bạn muốn linh hoạt\n",
        "\n",
        "        # ✅ Gán lại method nếu đang dùng FAISS custom\n",
        "        self.vectorstore.similarity_search_with_score_by_vector = MethodType(\n",
        "            similarity_search_with_score_by_vector,\n",
        "            self.vectorstore\n",
        "        )\n",
        "\n",
        "        print(f\"✅ FAISS index đã được load từ: {self.faiss_save_path}\")\n",
        "        return self.vectorstore\n",
        "\n",
        "    def query(self, question: str, k: int = 3):\n",
        "        \"\"\"Truy vấn câu hỏi vào FAISS và trả về top-k câu trả lời\"\"\"\n",
        "        if not self.vectorstore:\n",
        "            self.load_faiss_index()\n",
        "        if not hasattr(self.vectorstore, \"chunk_size\"):\n",
        "            self.vectorstore.chunk_size = 200\n",
        "        results = self.vectorstore.similarity_search(question, k=k)\n",
        "        return [clean_pdf_text(x.page_content) for x in results]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_db_index():\n",
        "  build_db = GenerateData('sentence-transformers/all-MiniLM-L6-v2', 'data_pdf')\n",
        "  build_db.load_pdf_documents()\n",
        "  build_db.build_faiss_index()\n",
        "  # return build_db.query('YOLO:BIT là gì?')\n",
        "\n",
        "build_db_index()"
      ],
      "metadata": {
        "id": "Dmx72NlQ_O-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22dab4f3-099e-481c-cea2-e3ff7ef91742"
      },
      "id": "Dmx72NlQ_O-f",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ FAISS index đã được lưu tại: ./faiss_index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a6955517",
      "metadata": {
        "id": "a6955517"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/model_cache/models--vilm--vinallama-7b-chat-GGUF/snapshots/5c76606edd7f6c714fba2988990dedecba33c0ff/vinallama-7b-chat_q5_0.gguf'\n",
        "\n",
        "\n",
        "def test_llm_conversation(firstTime = False, query = '', user_name ='', summary = False, apply_forget = False):\n",
        "\n",
        "  llm_client = LLMClient(user_name, model_path)\n",
        "  response = ''\n",
        "  if firstTime:\n",
        "    # memory_loader = MemoryForgetLoader('data/test_json.json')\n",
        "    memory_retrival = MemoryRetrival('sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                     5,\n",
        "                                     200,\n",
        "                                     'Khoa',\n",
        "                                     'data/test_json.json'\n",
        "                                     )\n",
        "    cur_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    memory_retrival.init_memory_index('data/test_json.json', 'index_storage', user_name, cur_date)\n",
        "    index = memory_retrival.load_memory_index(f'index_storage/{user_name}')\n",
        "\n",
        "    index.chunk_size = 200\n",
        "    personal_info = memory_retrival.search_memory(query, index, cur_date)\n",
        "\n",
        "\n",
        "    get_data = GenerateData('sentence-transformers/all-MiniLM-L6-v2', 'data_pdf')\n",
        "    data = get_data.query(query)\n",
        "\n",
        "\n",
        "    input = [\n",
        "        'personal_info',\n",
        "        'data',\n",
        "        'query'\n",
        "    ]\n",
        "\n",
        "    context = \"\"\"\n",
        "    Bạn là một trợ lý AI thông minh và thân thiện.\n",
        "\n",
        "    Thông tin cá nhân người dùng: {personal_info}\n",
        "    Thông tin liên quan để trả lời câu hỏi: {data}\n",
        "\n",
        "    Câu hỏi từ người dùng:\n",
        "    {query}\n",
        "\n",
        "    Dựa trên thông tin cá nhân và dữ kiện ở trên, hãy đưa ra câu trả lời phù hợp, đồng cảm và hữu ích.\n",
        "    \"\"\"\n",
        "    template = llm_client.create_template(input, context)\n",
        "    llm_client.build_model_with_template(template)\n",
        "    response = llm_client.generate_output(query, data, personal_info)\n",
        "\n",
        "  if summary:\n",
        "    llm_client.summary_user()\n",
        "\n",
        "  return response if response else 'Không có câu trả lời'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "863cb04a",
      "metadata": {
        "id": "863cb04a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55db0bd9-aacb-42e0-a420-451f74615e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-3201697175.py:6: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
            "  self.llm = LLMs(model_path)\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14542 MiB free\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/drive/MyDrive/model_cache/models--vilm--vinallama-7b-chat-GGUF/snapshots/5c76606edd7f6c714fba2988990dedecba33c0ff/vinallama-7b-chat_q5_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 8\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,46305]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,46305]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,46305]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,88120]   = [\"▁ đ\", \"n h\", \"▁n h\", \"▁ nh\",...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 46303\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q5_0:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q5_0\n",
            "print_info: file size   = 4.41 GiB (5.53 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: special tokens cache size = 5\n",
            "load: token to piece cache size = 0.2430 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 4096\n",
            "print_info: n_embd_v_gqa     = 4096\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 8B\n",
            "print_info: model params     = 6.86 B\n",
            "print_info: general.name     = LLaMA v2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 46305\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 46303 '<|im_end|>'\n",
            "print_info: EOT token        = 46303 '<|im_end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 2 '</s>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 46303 '<|im_end|>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q5_0) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
            "load_tensors: offloading 32 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 33/33 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =  4395.40 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   124.35 MiB\n",
            "...............................................................................................warning: failed to mlock 132870144-byte buffer (after previously locking 0 bytes): Cannot allocate memory\n",
            "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
            ".\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 2048\n",
            "llama_context: n_ctx_per_seq = 2048\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:  CUDA_Host  output buffer size =     0.18 MiB\n",
            "create_memory: n_ctx = 2048 (padded)\n",
            "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
            "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 2\n",
            "llama_context: max_nodes = 65536\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
            "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
            "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
            "llama_context:      CUDA0 compute buffer size =   164.00 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    12.01 MiB\n",
            "llama_context: graph nodes  = 1094\n",
            "llama_context: graph splits = 2\n",
            "CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '46303', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '8'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully write to data/test_json.json\n",
            "❌ Không có tài liệu nào được tạo.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open index_storage/Linh/index.faiss for reading: No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-1879560677.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_llm_conversation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'YOLO:BIT là gì?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Linh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirstTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-16-4170023493.py\u001b[0m in \u001b[0;36mtest_llm_conversation\u001b[0;34m(firstTime, query, user_name, summary, apply_forget)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcur_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmemory_retrival\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_memory_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/test_json.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index_storage'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_retrival\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_memory_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'index_storage/{user_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-3124628877.py\u001b[0m in \u001b[0;36mload_memory_index\u001b[0;34m(self, vs_path)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_memory_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;34m\"\"\"Tải FAISS index đã lưu từ vs_path\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         vector_store = FAISS.load_local(\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mvs_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36mload_local\u001b[0;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;31m# load index separately since it is not picklable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m         \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependable_faiss_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{index_name}.faiss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \u001b[0;31m# load docstore and index_to_docstore_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/faiss/swigfaiss_avx512.py\u001b[0m in \u001b[0;36mread_index\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m  11639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11640\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11641\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx512\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11643\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open index_storage/Linh/index.faiss for reading: No such file or directory"
          ]
        }
      ],
      "source": [
        "print(test_llm_conversation(query='YOLO:BIT là gì?', user_name='Linh', firstTime=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OBU8FWDsyP5j"
      },
      "id": "OBU8FWDsyP5j",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}