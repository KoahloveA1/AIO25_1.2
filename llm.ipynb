{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wqhasBqXoCYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb8840e-bb62-4ec1-d386-c4028e069fc1"
      },
      "id": "wqhasBqXoCYG",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "drive_cache = \"/content/drive/MyDrive/llama_cpp_build/llama_cpp\"\n",
        "site_pkg = \"/usr/local/lib/python3.11/dist-packages/llama_cpp\"\n",
        "\n",
        "\n",
        "if os.path.exists(drive_cache):\n",
        "    print(\"ƒêang copy llama_cpp t·ª´ Google Drive...\")\n",
        "    shutil.copytree(drive_cache, site_pkg, dirs_exist_ok=True)\n",
        "else:\n",
        "    print(\"Ch∆∞a c√≥ cache ‚Üí c√†i t·ª´ pip + CUDA\")\n",
        "    !CMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
        "    shutil.copytree(site_pkg, drive_cache, dirs_exist_ok=True)\n"
      ],
      "metadata": {
        "id": "OcNof5BbJlGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eadfe12-0c98-48a3-ad90-66359057287f"
      },
      "id": "OcNof5BbJlGN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ƒêang copy llama_cpp t·ª´ Google Drive...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirement.txt"
      ],
      "metadata": {
        "id": "dXQamG7Plq4B"
      },
      "id": "dXQamG7Plq4B",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "CACHE_DIR = \"/content/drive/MyDrive/model_cache\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# C·∫•u h√¨nh c√°c bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ cache transformers & llama.cpp\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = CACHE_DIR\n",
        "os.environ[\"HF_HOME\"] = CACHE_DIR\n",
        "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = CACHE_DIR\n",
        "os.environ[\"LLAMA_CPP_CACHE_DIR\"] = CACHE_DIR\n"
      ],
      "metadata": {
        "id": "yembEOseoQ1C"
      },
      "id": "yembEOseoQ1C",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = \"vinallama-7b-chat_q5_0.gguf\"\n",
        "model_path = os.path.join('/content/drive/MyDrive/model_cache/models--vilm--vinallama-7b-chat-GGUF/snapshots/5c76606edd7f6c714fba2988990dedecba33c0ff/', model_filename)\n",
        "\n",
        "if not os.path.isfile(model_path):\n",
        "    print(\"ƒêang t·∫£i m√¥ h√¨nh Q5_0‚Ä¶\")\n",
        "    !pip install -q huggingface_hub\n",
        "    from huggingface_hub import hf_hub_download\n",
        "    hf_hub_download(\n",
        "        repo_id=\"vilm/vinallama-7b-chat-GGUF\",\n",
        "        filename=model_filename,\n",
        "        cache_dir=CACHE_DIR,\n",
        "        library_name=\"llama.cpp\",\n",
        "        library_version=\"latest\"\n",
        "    )\n",
        "else:\n",
        "    print(\"M√¥ h√¨nh ƒë√£ t·ªìn t·∫°i, kh√¥ng t·∫£i l·∫°i.\")\n"
      ],
      "metadata": {
        "id": "-NKV971xoWzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d2da0a5-0eb9-457e-8232-b2862c815bce"
      },
      "id": "-NKV971xoWzJ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M√¥ h√¨nh ƒë√£ t·ªìn t·∫°i, kh√¥ng t·∫£i l·∫°i.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "53e4f5e7",
      "metadata": {
        "id": "53e4f5e7"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import copy\n",
        "import json\n",
        "\n",
        "\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "import datetime\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ba696d96",
      "metadata": {
        "id": "ba696d96"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/model_cache/models--vilm--vinallama-7b-chat-GGUF/snapshots/5c76606edd7f6c714fba2988990dedecba33c0ff/vinallama-7b-chat_q5_0.gguf'\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Gi·ªØ l·∫°i c·∫•u tr√∫c d·∫°ng ƒëo·∫°n vƒÉn ho·∫∑c li·ªát k√™.\n",
        "    X√≥a kho·∫£ng tr·∫Øng ƒë·∫ßu d√≤ng, d√≤ng tr·ªëng, v√† chu·∫©n h√≥a d√≤ng li√™n t·ª•c.\n",
        "    \"\"\"\n",
        "    # T√°ch t·ª´ng d√≤ng, lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi t·ª´ng d√≤ng\n",
        "    lines = [line.strip() for line in text.strip().splitlines()]\n",
        "\n",
        "    # Lo·∫°i b·ªè d√≤ng tr·ªëng\n",
        "    lines = [line for line in lines if line]\n",
        "\n",
        "    # N·∫øu to√†n b·ªô ch·ªâ l√† m·ªôt ƒëo·∫°n vƒÉn kh√¥ng xu·ªëng d√≤ng c√≥ ch·ªß √Ω, n·ªëi l·∫°i m·ªôt d√≤ng\n",
        "    if all(not re.match(r'^[-‚Ä¢*]|^[A-Z][a-z]+:', line) for line in lines):\n",
        "        return ' '.join(lines)\n",
        "\n",
        "    # Ng∆∞·ª£c l·∫°i: gi·ªØ xu·ªëng d√≤ng gi·ªØa c√°c ƒëo·∫°n c√≥ √Ω nghƒ©a\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "# Class kh·ªüi t·∫°o model llms\n",
        "class LLMs:\n",
        "    def __init__(self, model_id):\n",
        "        self.__model = LlamaCpp(\n",
        "            model_path=model_id,\n",
        "            n_gpu_layers=-1,\n",
        "            n_ctx=2048,\n",
        "            temperature=0.1,\n",
        "            max_tokens=256,\n",
        "            verbose=True,\n",
        "            n_batch=512,\n",
        "            n_threads=8,\n",
        "            use_mmap=True,\n",
        "            use_mlock=True,\n",
        "            seed=-1,\n",
        "            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
        "        )\n",
        "\n",
        "    def build_llm(self, prompt_template: PromptTemplate):\n",
        "        self.__llm_chain = LLMChain(llm=self.__model, prompt=prompt_template)\n",
        "        return self.__llm_chain\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.__model\n",
        "\n",
        "    def generate_output(self, input_dict: dict):\n",
        "        if not hasattr(self, '_LLMs__llm_chain'):\n",
        "            raise RuntimeError(\"LLM chain not built. Call build_llm() first.\")\n",
        "        raw_output = self.__llm_chain.run(input_dict)\n",
        "        return clean_text(raw_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "121d9d9f",
      "metadata": {
        "id": "121d9d9f"
      },
      "outputs": [],
      "source": [
        "def forgetting_curve(t, S):\n",
        "    \"\"\"\n",
        "    Calculate the retention of information at time t based on the forgetting curve.\n",
        "\n",
        "    :param t: Time elapsed since the information was learned (in days).\n",
        "    :type t: float\n",
        "    :param S: Strength of the memory.\n",
        "    :type S: float\n",
        "    :return: Retention of information at time t.\n",
        "    :rtype: float\n",
        "    Memory strength is a concept used in memory models to represent the durability or stability of a memory trace in the brain.\n",
        "    In the context of the forgetting curve, memory strength (denoted as 'S') is a parameter that\n",
        "    influences the rate at which information is forgotten.\n",
        "    The higher the memory strength, the slower the rate of forgetting,\n",
        "    and the longer the information is retained.\n",
        "    \"\"\"\n",
        "    return math.exp(-t / 5*S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6ba192b3",
      "metadata": {
        "id": "6ba192b3"
      },
      "outputs": [],
      "source": [
        "def get_docs_with_score(docs_with_score):\n",
        "    docs=[]\n",
        "    for doc, score in docs_with_score:\n",
        "        doc.metadata[\"score\"] = score\n",
        "        docs.append(doc)\n",
        "    return docs\n",
        "\n",
        "\n",
        "def seperate_list(ls: List[int]) -> List[List[int]]:\n",
        "    lists = []\n",
        "    ls1 = [ls[0]]\n",
        "    for i in range(1, len(ls)):\n",
        "        if ls[i-1] + 1 == ls[i]:\n",
        "            ls1.append(ls[i])\n",
        "        else:\n",
        "            lists.append(ls1)\n",
        "            ls1 = [ls[i]]\n",
        "    lists.append(ls1)\n",
        "    return lists\n",
        "\n",
        "\n",
        "def similarity_search_with_score_by_vector(\n",
        "        self,\n",
        "        embedding: List[float],\n",
        "        k: int = 4,\n",
        "        **kwargs\n",
        "    ) -> List[Tuple[Document, float]]:\n",
        "        scores, indices = self.index.search(np.array([embedding], dtype=np.float32), k)\n",
        "        docs = []\n",
        "        id_set = set()\n",
        "        for j, i in enumerate(indices[0]):\n",
        "            if i == -1:\n",
        "                # This happens when not enough docs are returned.\n",
        "                continue\n",
        "            _id = self.index_to_docstore_id[i]\n",
        "            doc = self.docstore.search(_id)\n",
        "            id_set.add(i)\n",
        "            docs_len = len(doc.page_content)\n",
        "            for k in range(1, max(i, len(self.index_to_docstore_id)-i)):\n",
        "                for l in [i+k, i-k]:\n",
        "                    if 0 <= l < len(self.index_to_docstore_id):\n",
        "                        _id0 = self.index_to_docstore_id[l]\n",
        "                        doc0 = self.docstore.search(_id0)\n",
        "                        # print(doc0.metadata)\n",
        "                        # exit()\n",
        "                        if docs_len + len(doc0.page_content) > self.chunk_size:\n",
        "                            break\n",
        "                        # print(doc0)\n",
        "                        elif doc0.metadata[\"source\"] == doc.metadata[\"source\"]:\n",
        "                            docs_len += len(doc0.page_content)\n",
        "                            id_set.add(l)\n",
        "        id_list = sorted(list(id_set))\n",
        "        id_lists = seperate_list(id_list)\n",
        "        for id_seq in id_lists:\n",
        "            for id in id_seq:\n",
        "                if id == id_seq[0]:\n",
        "                    _id = self.index_to_docstore_id[id]\n",
        "                    doc = self.docstore.search(_id)\n",
        "                else:\n",
        "                    _id0 = self.index_to_docstore_id[id]\n",
        "                    doc0 = self.docstore.search(_id0)\n",
        "                    doc.page_content += doc0.page_content\n",
        "            if not isinstance(doc, Document):\n",
        "                raise ValueError(f\"Could not find document for id {_id}, got {doc}\")\n",
        "            docs.append((doc, scores[0][j]))\n",
        "        return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2119a997",
      "metadata": {
        "id": "2119a997"
      },
      "outputs": [],
      "source": [
        "class MemoryForgetLoader:\n",
        "    # Kh·ªüi t·∫°o m·ªôt dict t·∫°m ƒë·ªÉ l∆∞u tr≈© c√°c th√¥ng tin.\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self.memory_bank = {}\n",
        "\n",
        "    def _get_date_difference(self, date1: str, date2: str) -> int:\n",
        "        date_format = \"%Y-%m-%d\"\n",
        "        d1 = datetime.datetime.strptime(date1, date_format)\n",
        "        d2 = datetime.datetime.strptime(date2, date_format)\n",
        "        return (d2 - d1).days\n",
        "\n",
        "\n",
        "    def write_memories(self, out_file):\n",
        "        with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            print(f'Successfully write to {out_file}')\n",
        "            json.dump(self.memory_bank, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    def load_memories(self, memory_file):\n",
        "        # print(memory_file)\n",
        "        with open(memory_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.memory_bank = json.load(f)\n",
        "    # H√†m kh·ªüi t·∫°o v√† c·∫≠p nh·∫≠t metadata v√†o db c√≥ k√®m theo c∆° ch·∫ø  suy gi·∫£m tr√≠ nh·ªõ.\n",
        "    def update_forget_memory(self, name, cur_date):\n",
        "        with open(self.file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        docs_about_user = []\n",
        "        for user_name, info in data.items():\n",
        "            self.memory_bank[user_name] = copy.deepcopy(info)\n",
        "            if user_name != name:\n",
        "                continue\n",
        "            if 'history' not in info.keys():\n",
        "                continue\n",
        "            for date, dialog in info['history'].items():\n",
        "                # Kh·ªüi t·∫°o m·∫£ng c√°c ƒëo·∫°n ƒë·ªëi tho·∫°i s·∫Ω b·ªã qu√™n c·ªßa user trong date\n",
        "                forget_index = []\n",
        "                tmp_str = f\"ƒê√¢y l√† ƒëo·∫°n ƒë·ªëi tho·∫°i v√†o {date} \"\n",
        "                for i, chat in enumerate(dialog):\n",
        "\n",
        "                    # Kh·ªüi t·∫°o metadata ho·∫∑c l·∫•y metadata ƒë·ªÉ tr√≠ch xu·∫•t qu√™n c·ªßa chabot v·ªÅ ƒëo·∫°n h·ªôi tho·∫°i n√†o ƒë√≥\n",
        "                    memory_strength = chat.get('memory_strength', 1)\n",
        "                    last_recall_date = chat.get('last_recall_date', date)\n",
        "                    memory_id = chat.get('memory_id', f'{user_name}_{date}_{i}')\n",
        "                    query = f\"[|User|]: {chat['query']}\"\n",
        "                    response = f\"[|AI|]: {chat['response']}\"\n",
        "                    tmp_str += query + response\n",
        "                    metadata = {\n",
        "                        'memory_strength': memory_strength,\n",
        "                        'last_recall_date': last_recall_date,\n",
        "                        'memory_id': memory_id\n",
        "                    }\n",
        "                    self.memory_bank[user_name]['history'][date][i].update(metadata)\n",
        "\n",
        "                    diff_date = self._get_date_difference(last_recall_date, cur_date)\n",
        "                    forget_probability = forgetting_curve(diff_date, memory_strength)\n",
        "                    # Th·ª≠ kh·∫£ nƒÉng li·ªáu c√≥ qu√™n hay kh√¥ng?\n",
        "                    if random.random() > forget_probability:\n",
        "                        forget_index.append(i)\n",
        "                    else:\n",
        "                        # L∆∞u th√¥ng tin ng∆∞·ªùi d√πng ƒë·ªÉ t·∫°o documents v·ªõi metadata ƒë·ªÉ d·ªÖ retrival\n",
        "                        docs_about_user.append(Document(page_content=tmp_str,metadata=metadata))\n",
        "                if len(forget_index) > 0:\n",
        "                    forget_index.sort(reverse=True)\n",
        "                    for idd in forget_index:\n",
        "                        self.memory_bank[user_name]['history'][date].pop(idd)\n",
        "                        print(f'Delete convestion of {user_name} on {date}')\n",
        "                if len(self.memory_bank[user_name]['history'][date]) == 0:\n",
        "                    self.memory_bank[user_name]['history'].pop(date)\n",
        "                    self.memory_bank[user_name]['summary'].pop(date)\n",
        "\n",
        "                if 'summary' in info.keys():\n",
        "                    if date in self.memory_bank[user_name]['summary'].keys():\n",
        "                        summary = f\"ƒê√¢y l√† t√≥m t·∫Øt v√†o ng√†y {date}\"\n",
        "                        summary += data[user_name]['summary'][date]['content']\n",
        "                        memory_strength = self.memory_bank[user_name]['summary'][date].get('memory_strength',1)\n",
        "                        last_recall_date = self.memory_bank[user_name][\"summary\"][date].get('last_recall_date',date)\n",
        "                        metadata = {\n",
        "                            'memory_strength':memory_strength,\n",
        "                            'memory_id':f'{user_name}_{date}_summary',\n",
        "                            'last_recall_date':last_recall_date,\"source\":f'{user_name}_{date}_summary'\n",
        "                        }\n",
        "                        self.memory_bank[user_name]['summary'][date].update(metadata)\n",
        "                        docs_about_user.append(Document(page_content=summary,metadata=metadata))\n",
        "                # if 'overall_history' in info.keys():\n",
        "                #     metadata = {\n",
        "                #         'overall_history' : user_name\n",
        "                #     }\n",
        "                #     docs_about_user.append(Document(page_content=data[user_name]['overall_history'], metadata=metadata))\n",
        "                # if 'overall_personality' in info.keys():\n",
        "                #     metadata = {\n",
        "                #         'overall_personality' : user_name\n",
        "                #     }\n",
        "                #     docs_about_user.append(Document(page_content=data[user_name]['overall_personality'], metadata=metadata))\n",
        "        self.write_memories(self.file_path)\n",
        "        return docs_about_user\n",
        "\n",
        "    def update_memory_when_searched(self, recalled_memos,user,cur_date):\n",
        "        for recalled in recalled_memos:\n",
        "            recalled_id = recalled.metadata['memory_id']\n",
        "            recalled_date = recalled_id.split('_')[1]\n",
        "            for i,memory in enumerate(self.memory_bank[user]['history'][recalled_date]):\n",
        "                if memory['memory_id'] == recalled_id:\n",
        "                    self.memory_bank[user]['history'][recalled_date][i]['memory_strength'] += 1\n",
        "                    self.memory_bank[user]['history'][recalled_date][i]['last_recall_date'] = cur_date\n",
        "                    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a1387ecc",
      "metadata": {
        "id": "a1387ecc"
      },
      "outputs": [],
      "source": [
        "# Class kh·ªüi t·∫°o m·ªôt instance c√≥ kh·∫£ nƒÉng truy xu·∫•t b·ªô nh·ªõ.\n",
        "from types import MethodType\n",
        "class MemoryRetrival:\n",
        "    def __init__(self, embedding_name, top_k, chunk_size, user, file_path):\n",
        "        self.embedding_model = HuggingFaceEmbeddings(model_name=embedding_name)\n",
        "        self.top_k = top_k\n",
        "        self.chunk_size = chunk_size\n",
        "        self.user = user\n",
        "        self.memory_path = file_path\n",
        "\n",
        "    def helper_load_file(self, file_name: str, user_name: str, cur_date):\n",
        "        loader = MemoryForgetLoader(file_name)\n",
        "        docs = loader.update_forget_memory(user_name,cur_date)\n",
        "        splitter = RecursiveCharacterTextSplitter()\n",
        "        docs = splitter.split_documents(docs)\n",
        "        return docs, loader\n",
        "\n",
        "    def init_memory_index(self, file_name: str, saving_path: str, user_name: str, cur_date: str):\n",
        "        # Load v√† chunk vƒÉn b·∫£n\n",
        "        docs, self.memory_loader = self.helper_load_file(file_name, user_name, cur_date)\n",
        "\n",
        "        if not docs:\n",
        "            print(\"‚ùå Kh√¥ng c√≥ t√†i li·ªáu n√†o ƒë∆∞·ª£c t·∫°o.\")\n",
        "            return\n",
        "\n",
        "        # T·∫°o FAISS index t·ª´ t√†i li·ªáu\n",
        "        vector_store = FAISS.from_documents(docs, self.embedding_model)\n",
        "\n",
        "        # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i\n",
        "        os.makedirs(saving_path, exist_ok=True)\n",
        "\n",
        "        # ƒê·∫∑t t√™n file FAISS theo user_name\n",
        "        file_id = user_name or f\"memory_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        full_path = os.path.join(saving_path, file_id)\n",
        "\n",
        "        # L∆∞u FAISS\n",
        "        vector_store.save_local(full_path)\n",
        "        print(f\"üì¶ FAISS index ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {full_path}\")\n",
        "\n",
        "\n",
        "    def load_memory_index(self, vs_path: str):\n",
        "        \"\"\"T·∫£i FAISS index ƒë√£ l∆∞u t·ª´ vs_path\"\"\"\n",
        "        vector_store = FAISS.load_local(\n",
        "            vs_path,\n",
        "            self.embedding_model,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "\n",
        "        # G√°n h√†m t√¨m ki·∫øm custom n·∫øu c√≥ (t·ªëi ∆∞u gh√©p chunk)\n",
        "        FAISS.similarity_search_with_score_by_vector = MethodType(similarity_search_with_score_by_vector, vector_store)\n",
        "\n",
        "        # G√°n chunk_size cho vector_store ƒë·ªÉ s·ª≠ d·ª•ng trong search n·∫øu c·∫ßn\n",
        "        vector_store.chunk_size = self.chunk_size\n",
        "\n",
        "        print(f\"‚úÖ ƒê√£ load memory index t·ª´: {vs_path}\")\n",
        "        return vector_store\n",
        "\n",
        "\n",
        "    def search_memory(self, query: str, vector_store, cur_date=''):\n",
        "        related_docs_with_score = vector_store.similarity_search_with_score(query, k=self.top_k)\n",
        "        related_docs = get_docs_with_score(related_docs_with_score)\n",
        "\n",
        "        # ‚úÖ T√°ch ng√†y t·ª´ memory_id ƒë·ªÉ s·∫Øp theo ng√†y + memory_id\n",
        "        def extract_date(doc):\n",
        "            memory_id = doc.metadata[\"memory_id\"]\n",
        "            return memory_id.split(\"_\")[1]  # v√≠ d·ª•: \"2025-06-25\"\n",
        "\n",
        "        related_docs = sorted(\n",
        "            related_docs,\n",
        "            key=lambda x: (extract_date(x), x.metadata[\"memory_id\"])\n",
        "        )\n",
        "\n",
        "        pre_date = ''\n",
        "        date_docs = []\n",
        "        dates = []\n",
        "        cur_date = cur_date if cur_date else datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        for doc in related_docs:\n",
        "            date_str = extract_date(doc)\n",
        "            doc.page_content = doc.page_content.replace(f'ƒê√¢y l√† ƒëo·∫°n ƒë·ªëi tho·∫°i v√†o {date_str}Ôºö', '').strip()\n",
        "\n",
        "            if date_str != pre_date:\n",
        "                date_docs.append(doc.page_content)\n",
        "                pre_date = date_str\n",
        "                dates.append(pre_date)\n",
        "            else:\n",
        "                date_docs[-1] += f'\\n{doc.page_content}'\n",
        "\n",
        "        self.memory_loader.update_memory_when_searched(related_docs, user=self.user, cur_date=cur_date)\n",
        "        self.save_updated_memory()\n",
        "        return date_docs, ', '.join(dates)\n",
        "\n",
        "    def save_updated_memory(self):\n",
        "        self.memory_loader.write_memories(self.memory_path)#.replace('.json','_forget_format.json'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4e8d2d10",
      "metadata": {
        "id": "4e8d2d10"
      },
      "outputs": [],
      "source": [
        "class LLMClientSimple:\n",
        "    def __init__(self, llm_chain):\n",
        "        self.llm_chain = llm_chain  # LLMChain t·ª´ m√¥ h√¨nh local ƒë√£ ƒë∆∞·ª£c build\n",
        "\n",
        "    def generate_text_simple(self, prompt, prompt_num=1, language='vi'):\n",
        "        \"\"\"\n",
        "        Sinh vƒÉn b·∫£n t·ª´ local model v·ªõi prompt ƒë·∫ßu v√†o.\n",
        "        prompt_num kh√¥ng √°p d·ª•ng v·ªõi local LLM, ch·ªâ gi·ªØ l·∫°i cho t∆∞∆°ng th√≠ch.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = self.llm_chain.run({\"text\": prompt})\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"L·ªói khi sinh vƒÉn b·∫£n t·ª´ local model: {e}\")\n",
        "            return \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9eb8e870",
      "metadata": {
        "id": "9eb8e870"
      },
      "outputs": [],
      "source": [
        "# T·∫°o prompt cho t√≥m t·∫Øt\n",
        "\n",
        "def summarize_content_prompt(content, user_name, bot_name):\n",
        "    prompt = \"H√£y t√≥m t·∫Øt n·ªôi dung cu·ªôc h·ªôi tho·∫°i sau b·∫±ng ti·∫øng Vi·ªát, r√∫t ra ch·ªß ƒë·ªÅ ch√≠nh v√† nh·ªØng th√¥ng tin quan tr·ªçng:\\n\"\n",
        "    for dialog in content:\n",
        "        prompt += f\"{user_name}: {dialog['query'].strip()}\\n\"\n",
        "        prompt += f\"{bot_name}: {dialog['response'].strip()}\\n\"\n",
        "    prompt += \"T√≥m t·∫Øt:\\n\"\n",
        "    return prompt\n",
        "\n",
        "def summarize_overall_prompt(content):\n",
        "    prompt = \"H√£y t√≥m t·∫Øt ng·∫Øn g·ªçn nh·ªØng s·ª± ki·ªán ƒë√£ di·ªÖn ra d∆∞·ªõi ƒë√¢y, ch·ªâ gi·ªØ l·∫°i c√°c th√¥ng tin quan tr·ªçng nh·∫•t:\\n\"\n",
        "    for date, summary_dict in content:\n",
        "        summary = summary_dict['content']\n",
        "        prompt += f\"- Ng√†y {date}: {summary.strip()}\\n\"\n",
        "    prompt += \"T√≥m t·∫Øt t·ªïng qu√°t:\\n\"\n",
        "    return prompt\n",
        "\n",
        "def summarize_personality_prompt(content, user_name, bot_name):\n",
        "    prompt = f\"H√£y d·ª±a v√†o ƒëo·∫°n h·ªôi tho·∫°i sau ƒë·ªÉ ph√¢n t√≠ch t√≠nh c√°ch v√† c·∫£m x√∫c c·ªßa {user_name}, ƒë·ªìng th·ªùi ƒë·ªÅ xu·∫•t chi·∫øn l∆∞·ª£c ph·∫£n h·ªìi ph√π h·ª£p cho {bot_name}:\\n\"\n",
        "    for dialog in content:\n",
        "        prompt += f\"{user_name}: {dialog['query'].strip()}\\n\"\n",
        "        prompt += f\"{bot_name}: {dialog['response'].strip()}\\n\"\n",
        "    prompt += f\"\\nT√≠nh c√°ch, c·∫£m x√∫c c·ªßa {user_name} v√† chi·∫øn l∆∞·ª£c ph·∫£n h·ªìi c·ªßa {bot_name} l√†:\\n\"\n",
        "    return prompt\n",
        "\n",
        "def summarize_overall_personality(content):\n",
        "    prompt = \"D∆∞·ªõi ƒë√¢y l√† c√°c ph√¢n t√≠ch v·ªÅ t√≠nh c√°ch v√† c·∫£m x√∫c ng∆∞·ªùi d√πng trong nhi·ªÅu ƒëo·∫°n h·ªôi tho·∫°i:\\n\"\n",
        "    for date, summary in content:\n",
        "        prompt += f\"- Ng√†y {date}: {summary.strip()}\\n\"\n",
        "    prompt += \"\\nVui l√≤ng t·ªïng h·ª£p th√†nh m·ªôt b·∫£n t√≥m t·∫Øt ng·∫Øn g·ªçn v·ªÅ t√≠nh c√°ch t·ªïng th·ªÉ c·ªßa ng∆∞·ªùi d√πng v√† c√°ch ph·∫£n h·ªìi ph√π h·ª£p nh·∫•t t·ª´ AI:\\n\"\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7f3bdb02",
      "metadata": {
        "id": "7f3bdb02"
      },
      "outputs": [],
      "source": [
        "# T√≥m t·∫Øt l·∫°i c√°c ƒëo·∫°n ƒë·ªôi tho·∫°i c·ªßa user_name t∆∞∆°ng ·ª©ng v√† l∆∞u v√†o file db\n",
        "\n",
        "def summarize_memory(memory_path, name, llm_client):\n",
        "    bot_name = \"AI\"\n",
        "    gen_prompt_num = 1\n",
        "    with open(memory_path, 'r', encoding='utf8') as f:\n",
        "        memory = json.load(f)\n",
        "\n",
        "    for user_name, user_data in memory.items():\n",
        "        if name is not None and user_name != name:\n",
        "            continue\n",
        "\n",
        "        print(f\"Updating memory for user: {user_name}\")\n",
        "\n",
        "        history = user_data.get(\"history\", {})\n",
        "        if not history:\n",
        "            continue\n",
        "\n",
        "        user_data.setdefault(\"summary\", {})\n",
        "        user_data.setdefault(\"personality\", {})\n",
        "\n",
        "        for date, content in history.items():\n",
        "            summary_exists = bool(user_data[\"summary\"].get(date))\n",
        "            personality_exists = bool(user_data[\"personality\"].get(date))\n",
        "\n",
        "            content_prompt = summarize_content_prompt(content, user_name, bot_name)\n",
        "            personality_prompt = summarize_personality_prompt(content, user_name, bot_name)\n",
        "\n",
        "            if not summary_exists:\n",
        "                summary_text = llm_client.generate_text_simple(prompt=content_prompt, prompt_num=gen_prompt_num, language=\"vi\")\n",
        "                user_data[\"summary\"][date] = {\"content\": summary_text}\n",
        "\n",
        "            if not personality_exists:\n",
        "                personality_text = llm_client.generate_text_simple(prompt=personality_prompt, prompt_num=gen_prompt_num, language=\"vi\")\n",
        "                user_data[\"personality\"][date] = personality_text\n",
        "\n",
        "        overall_content_prompt = summarize_overall_prompt(list(user_data[\"summary\"].items()))\n",
        "        overall_personality_prompt = summarize_overall_personality(list(user_data[\"personality\"].items()))\n",
        "\n",
        "        user_data[\"overall_history\"] = llm_client.generate_text_simple(prompt=overall_content_prompt, prompt_num=gen_prompt_num, language=\"vi\")\n",
        "        user_data[\"overall_personality\"] = llm_client.generate_text_simple(prompt=overall_personality_prompt, prompt_num=gen_prompt_num, language=\"vi\")\n",
        "\n",
        "    with open(memory_path, 'w', encoding='utf8') as f:\n",
        "        json.dump(memory, f, ensure_ascii=False, indent=4)\n",
        "        print(f\"Memory updated for {'all users' if name is None else name}\")\n",
        "\n",
        "    return memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "11b8ef87",
      "metadata": {
        "id": "11b8ef87"
      },
      "outputs": [],
      "source": [
        "# Class application ƒë∆∞·ª£c d√πng khi ƒë√£ t√≠ch h·ª£p ƒë·∫ßy ƒë·ªß c√°c c∆° ch·∫ø\n",
        "class LLMClient:\n",
        "    def __init__(self, user_name=None, model_path='../models/vinallama-7b-chat_q5_0.gguf'):\n",
        "        self.user_name = user_name\n",
        "        self.file_path_db = os.path.join(os.getcwd(), \"data/test_json.json\")\n",
        "        self.llm = LLMs(model_path)\n",
        "\n",
        "    def create_template(self, data: List[str], instruction: str):\n",
        "        \"\"\"\n",
        "        T·∫°o prompt_template v·ªõi:\n",
        "            data: l√† 1 list str bao g·ªìm c√°c d·ªØ li·ªáu nh∆∞ th√¥ng tin v·ªÅ  AI ghi nh·ªõ v√† d·ªØ li·ªáu mu·ªën AI tr·∫£ l·ªùi\n",
        "            instruction: H∆∞·ªõng d·∫´n chatbot tr·∫£ l·ªùi theo format mong mu·ªën\n",
        "        \"\"\"\n",
        "        template = PromptTemplate(\n",
        "            input_variables = data,\n",
        "            template=instruction\n",
        "        )\n",
        "        return template\n",
        "\n",
        "    def build_model_with_template(self, prompt_template):\n",
        "        self.chatbot = self.llm.build_llm(prompt_template)\n",
        "    # Need to update\n",
        "    def summary_user(self):\n",
        "        default_prompt = PromptTemplate(\n",
        "        input_variables=[\"text\"],\n",
        "          template=\"\"\"\n",
        "      B·∫°n l√† m·ªôt tr·ª£ l√Ω AI l·ªãch s·ª±, r√µ r√†ng. Vui l√≤ng kh√¥ng s·ª≠ d·ª•ng k√Ω t·ª± xu·ªëng d√≤ng, k√Ω t·ª± ƒë·∫∑c bi·ªát nh∆∞ \"\\\\n\", \"\\\\t\" hay ƒë·ªãnh d·∫°ng Markdown. Tr·∫£ l·ªùi tr·ª±c ti·∫øp v√† ng·∫Øn g·ªçn, tr√™n m·ªôt d√≤ng duy nh·∫•t.\n",
        "\n",
        "      C√¢u h·ªèi: {text}\n",
        "      Tr·∫£ l·ªùi:\n",
        "      \"\"\"\n",
        "      )\n",
        "        print('C·∫≠p nh·∫≠t th√¥ng tin user')\n",
        "        tmp_model = self.llm.build_llm(default_prompt)\n",
        "        client_simple = LLMClientSimple(tmp_model)\n",
        "        summarize_memory(self.file_path_db, self.user_name, client_simple)\n",
        "\n",
        "\n",
        "    def add_conservation_to_db(self, chat: dict):\n",
        "        \"\"\"\n",
        "        Th√™m m·ªôt ƒëo·∫°n h·ªôi tho·∫°i v√†o database JSON hi·ªán c√≥ (·ªü self.file_path_db).\n",
        "        N·∫øu file ch∆∞a t·ªìn t·∫°i th√¨ t·∫°o m·ªõi.\n",
        "        D·ªØ li·ªáu ƒë·∫ßu v√†o ph·∫£i theo format:\n",
        "        {\n",
        "            \"2025-07-01\": [  # ng√†y\n",
        "                {\n",
        "                    \"query\": \"...\",\n",
        "                    \"response\": \"...\",\n",
        "                    \"memory_strength\": ...,\n",
        "                    \"last_recall_date\": \"...\",\n",
        "                    \"memory_id\": \"...\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        # ƒê·ªçc d·ªØ li·ªáu hi·ªán c√≥\n",
        "        if os.path.exists(self.file_path_db):\n",
        "            with open(self.file_path_db, 'r', encoding='utf-8') as f:\n",
        "                try:\n",
        "                    data = json.load(f)\n",
        "                except json.JSONDecodeError:\n",
        "                    data = {}\n",
        "        else:\n",
        "            data = {}\n",
        "\n",
        "        # Kh·ªüi t·∫°o user n·∫øu ch∆∞a c√≥\n",
        "        if self.user_name not in data:\n",
        "            data[self.user_name] = {\n",
        "                \"history\": {},\n",
        "                \"summary\": {},\n",
        "                \"personality\": {},\n",
        "                \"overall_history\": \"\",\n",
        "                \"overall_personality\": \"\"\n",
        "            }\n",
        "\n",
        "        # Th√™m chat v√†o ph·∫ßn history\n",
        "        for date, messages in chat.items():\n",
        "            if \"history\" not in data[self.user_name]:\n",
        "                data[self.user_name][\"history\"] = {}\n",
        "\n",
        "            if date not in data[self.user_name][\"history\"]:\n",
        "                data[self.user_name][\"history\"][date] = []\n",
        "\n",
        "            if isinstance(messages, list):\n",
        "                data[self.user_name][\"history\"][date].extend(messages)\n",
        "\n",
        "        # Ghi l·∫°i v√†o file\n",
        "        with open(self.file_path_db, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    def apply_forget(self):\n",
        "      memory_loader = MemoryForgetLoader(self.file_path_db)\n",
        "      memory_loader.update_forget_memory(self.user_name, datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "\n",
        "    def generate_output(self, query: str, data, personal_info) -> str:\n",
        "        \"\"\"\n",
        "        Sinh c√¢u tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh v·ªõi c√¢u h·ªèi `query` v√† ng·ªØ c·∫£nh `context` (n·∫øu c√≥),\n",
        "        sau ƒë√≥ l∆∞u ƒëo·∫°n h·ªôi tho·∫°i v√†o database JSON.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"chatbot\"):\n",
        "            raise RuntimeError(\"Model ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o. H√£y g·ªçi build_model_with_template() tr∆∞·ªõc.\")\n",
        "\n",
        "        # Chu·∫©n b·ªã input cho LLMChain\n",
        "        input_data = {\"personal_info\" : personal_info,\n",
        "                      \"data\" : data,\n",
        "                      \"query\" : query\n",
        "                      }\n",
        "\n",
        "        # Sinh c√¢u tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh\n",
        "        response = self.llm.generate_output(input_data)\n",
        "\n",
        "        # L·∫•y ng√†y h√¥m nay d·∫°ng yyyy-mm-dd\n",
        "        today = datetime.date.today().isoformat()\n",
        "\n",
        "        # T·∫°o ƒëo·∫°n h·ªôi tho·∫°i ƒë·ªÉ l∆∞u\n",
        "        new_entry = {\n",
        "            \"query\": query,\n",
        "            \"response\": response,\n",
        "            \"memory_strength\": 1,  # c√≥ th·ªÉ thay ƒë·ªïi theo logic AI\n",
        "            \"last_recall_date\": today,\n",
        "            \"memory_id\": f\"{self.user_name}_{today}_{int(datetime.datetime.now().timestamp())}\"\n",
        "        }\n",
        "\n",
        "        # ƒê√≥ng g√≥i th√†nh dict ƒë·ªÉ l∆∞u v√†o DB\n",
        "        chat_to_save = {\n",
        "            \"history\": {\n",
        "                today: [new_entry]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # L∆∞u v√†o JSON DB\n",
        "        self.add_conservation_to_db(chat_to_save)\n",
        "\n",
        "        return response\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "86406df3",
      "metadata": {
        "id": "86406df3"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "def clean_pdf_text(text: str) -> str:\n",
        "    lines = text.splitlines()\n",
        "    cleaned_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        # Lo·∫°i b·ªè c√°c d√≤ng ch·ª©a s·ªë trang ho·∫∑c header/footer l·∫∑p l·∫°i\n",
        "        if re.search(r'Page \\d+/\\d+', line):\n",
        "            continue\n",
        "        if \"Tr∆∞·ªùng ƒê·∫°i H·ªçc B√°ch Khoa\" in line:\n",
        "            continue\n",
        "        if \"Khoa Khoa H·ªçc V√† Kƒ© Thu·∫≠t M√°y T√≠nh\" in line:\n",
        "            continue\n",
        "        if \"Assignment Software Engineering\" in line:\n",
        "            continue\n",
        "        cleaned_lines.append(line.strip())\n",
        "\n",
        "    return \"\\n\".join(cleaned_lines)\n",
        "class GenerateData:\n",
        "    def __init__(self, embedding_name: str, folder_data: str, faiss_save_path: str = \"./faiss_index\"):\n",
        "        self.model_embedding = HuggingFaceEmbeddings(model_name=embedding_name)\n",
        "        self.folder_data = folder_data\n",
        "        self.faiss_save_path = faiss_save_path\n",
        "        self.documents = []\n",
        "        self.vectorstore = None\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=512,\n",
        "            chunk_overlap=100\n",
        "        )\n",
        "    def load_pdf_documents(self):\n",
        "        \"\"\"ƒê·ªçc m·ªói file PDF th√†nh 1 Document duy nh·∫•t, ch·ªâ l∆∞u metadata l√† t√™n file\"\"\"\n",
        "        for filename in os.listdir(self.folder_data):\n",
        "            if filename.lower().endswith(\".pdf\"):\n",
        "                filepath = os.path.join(self.folder_data, filename)\n",
        "                try:\n",
        "                    with open(filepath, \"rb\") as f:\n",
        "                        reader = PyPDF2.PdfReader(f)\n",
        "                        text = \"\"\n",
        "                        for page in reader.pages:\n",
        "                            text += page.extract_text() or \"\"\n",
        "                    if text.strip():\n",
        "                        metadata = {\"source\": filename}  # ‚úÖ ch·ªâ l∆∞u t√™n file\n",
        "                        self.documents.append(Document(page_content=text, metadata=metadata))\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå L·ªói ƒë·ªçc {filename}: {e}\")\n",
        "        return self.documents\n",
        "\n",
        "    def split_documents(self):\n",
        "        \"\"\"T√°ch nh·ªè t√†i li·ªáu th√†nh c√°c chunk ƒë·ªÉ ƒë∆∞a v√†o FAISS\"\"\"\n",
        "        if not self.documents:\n",
        "            self.load_pdf_documents()\n",
        "        return self.text_splitter.split_documents(self.documents)\n",
        "\n",
        "    def build_faiss_index(self, save: bool = True):\n",
        "        \"\"\"T·∫°o FAISS index t·ª´ c√°c document ƒë√£ split\"\"\"\n",
        "        docs = self.split_documents()\n",
        "        self.vectorstore = FAISS.from_documents(docs, self.model_embedding)\n",
        "\n",
        "        if save:\n",
        "            self.vectorstore.save_local(self.faiss_save_path)\n",
        "            print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {self.faiss_save_path}\")\n",
        "        return self.vectorstore\n",
        "\n",
        "    def load_faiss_index(self):\n",
        "        \"\"\"Load FAISS index t·ª´ th∆∞ m·ª•c ƒë√£ l∆∞u\"\"\"\n",
        "        self.vectorstore = FAISS.load_local(\n",
        "            self.faiss_save_path,\n",
        "            self.model_embedding,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "\n",
        "        # ‚úÖ G√°n chunk_size v√†o vectorstore\n",
        "        self.vectorstore.chunk_size = 200  # ho·∫∑c truy·ªÅn t·ª´ self n·∫øu b·∫°n mu·ªën linh ho·∫°t\n",
        "\n",
        "        # ‚úÖ G√°n l·∫°i method n·∫øu ƒëang d√πng FAISS custom\n",
        "        self.vectorstore.similarity_search_with_score_by_vector = MethodType(\n",
        "            similarity_search_with_score_by_vector,\n",
        "            self.vectorstore\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c load t·ª´: {self.faiss_save_path}\")\n",
        "        return self.vectorstore\n",
        "\n",
        "    def query(self, question: str, k: int = 3):\n",
        "        \"\"\"Truy v·∫•n c√¢u h·ªèi v√†o FAISS v√† tr·∫£ v·ªÅ top-k c√¢u tr·∫£ l·ªùi\"\"\"\n",
        "        if not self.vectorstore:\n",
        "            self.load_faiss_index()\n",
        "        if not hasattr(self.vectorstore, \"chunk_size\"):\n",
        "            self.vectorstore.chunk_size = 200\n",
        "        results = self.vectorstore.similarity_search(question, k=k)\n",
        "        return [clean_pdf_text(x.page_content) for x in results]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_db_index():\n",
        "  build_db = GenerateData('sentence-transformers/all-MiniLM-L6-v2', 'data_pdf')\n",
        "  build_db.load_pdf_documents()\n",
        "  build_db.build_faiss_index()\n",
        "  # return build_db.query('YOLO:BIT l√† g√¨?')\n",
        "\n",
        "build_db_index()"
      ],
      "metadata": {
        "id": "Dmx72NlQ_O-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22dab4f3-099e-481c-cea2-e3ff7ef91742"
      },
      "id": "Dmx72NlQ_O-f",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FAISS index ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: ./faiss_index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a6955517",
      "metadata": {
        "id": "a6955517"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/model_cache/models--vilm--vinallama-7b-chat-GGUF/snapshots/5c76606edd7f6c714fba2988990dedecba33c0ff/vinallama-7b-chat_q5_0.gguf'\n",
        "\n",
        "\n",
        "def test_llm_conversation(firstTime = False, query = '', user_name ='', summary = False, apply_forget = False):\n",
        "\n",
        "  llm_client = LLMClient(user_name, model_path)\n",
        "  response = ''\n",
        "  if firstTime:\n",
        "    # memory_loader = MemoryForgetLoader('data/test_json.json')\n",
        "    memory_retrival = MemoryRetrival('sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                     5,\n",
        "                                     200,\n",
        "                                     'Khoa',\n",
        "                                     'data/test_json.json'\n",
        "                                     )\n",
        "    cur_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    memory_retrival.init_memory_index('data/test_json.json', 'index_storage', user_name, cur_date)\n",
        "    index = memory_retrival.load_memory_index(f'index_storage/{user_name}')\n",
        "\n",
        "    index.chunk_size = 200\n",
        "    personal_info = memory_retrival.search_memory(query, index, cur_date)\n",
        "\n",
        "\n",
        "    get_data = GenerateData('sentence-transformers/all-MiniLM-L6-v2', 'data_pdf')\n",
        "    data = get_data.query(query)\n",
        "\n",
        "\n",
        "    input = [\n",
        "        'personal_info',\n",
        "        'data',\n",
        "        'query'\n",
        "    ]\n",
        "\n",
        "    context = \"\"\"\n",
        "    B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh v√† th√¢n thi·ªán.\n",
        "\n",
        "    Th√¥ng tin c√° nh√¢n ng∆∞·ªùi d√πng: {personal_info}\n",
        "    Th√¥ng tin li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi: {data}\n",
        "\n",
        "    C√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng:\n",
        "    {query}\n",
        "\n",
        "    D·ª±a tr√™n th√¥ng tin c√° nh√¢n v√† d·ªØ ki·ªán ·ªü tr√™n, h√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ph√π h·ª£p, ƒë·ªìng c·∫£m v√† h·ªØu √≠ch.\n",
        "    \"\"\"\n",
        "    template = llm_client.create_template(input, context)\n",
        "    llm_client.build_model_with_template(template)\n",
        "    response = llm_client.generate_output(query, data, personal_info)\n",
        "\n",
        "  if summary:\n",
        "    llm_client.summary_user()\n",
        "\n",
        "  return response if response else 'Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "863cb04a",
      "metadata": {
        "id": "863cb04a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55db0bd9-aacb-42e0-a420-451f74615e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-3201697175.py:6: DeprecationWarning: callback_manager is deprecated. Please use callbacks instead.\n",
            "  self.llm = LLMs(model_path)\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14542 MiB free\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /content/drive/MyDrive/model_cache/models--vilm--vinallama-7b-chat-GGUF/snapshots/5c76606edd7f6c714fba2988990dedecba33c0ff/vinallama-7b-chat_q5_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 8\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,46305]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,46305]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,46305]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,88120]   = [\"‚ñÅ ƒë\", \"n h\", \"‚ñÅn h\", \"‚ñÅ nh\",...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 46303\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q5_0:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q5_0\n",
            "print_info: file size   = 4.41 GiB (5.53 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: special tokens cache size = 5\n",
            "load: token to piece cache size = 0.2430 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 4096\n",
            "print_info: n_embd_v_gqa     = 4096\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 11008\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 8B\n",
            "print_info: model params     = 6.86 B\n",
            "print_info: general.name     = LLaMA v2\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 46305\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 46303 '<|im_end|>'\n",
            "print_info: EOT token        = 46303 '<|im_end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 2 '</s>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 46303 '<|im_end|>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CUDA0, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q5_0) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
            "load_tensors: offloading 32 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 33/33 layers to GPU\n",
            "load_tensors:        CUDA0 model buffer size =  4395.40 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =   124.35 MiB\n",
            "...............................................................................................warning: failed to mlock 132870144-byte buffer (after previously locking 0 bytes): Cannot allocate memory\n",
            "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
            ".\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 2048\n",
            "llama_context: n_ctx_per_seq = 2048\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:  CUDA_Host  output buffer size =     0.18 MiB\n",
            "create_memory: n_ctx = 2048 (padded)\n",
            "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
            "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
            "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  11: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  12: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  13: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  14: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  15: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  16: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  17: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  18: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  19: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  20: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  21: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  22: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  23: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  24: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  25: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  26: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  27: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  28: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  29: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  30: dev = CUDA0\n",
            "llama_kv_cache_unified: layer  31: dev = CUDA0\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_kv_cache_unified: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 2\n",
            "llama_context: max_nodes = 65536\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
            "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
            "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
            "llama_context:      CUDA0 compute buffer size =   164.00 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    12.01 MiB\n",
            "llama_context: graph nodes  = 1094\n",
            "llama_context: graph splits = 2\n",
            "CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '46303', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '8'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully write to data/test_json.json\n",
            "‚ùå Kh√¥ng c√≥ t√†i li·ªáu n√†o ƒë∆∞·ª£c t·∫°o.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open index_storage/Linh/index.faiss for reading: No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-1879560677.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_llm_conversation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'YOLO:BIT l√† g√¨?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Linh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirstTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-16-4170023493.py\u001b[0m in \u001b[0;36mtest_llm_conversation\u001b[0;34m(firstTime, query, user_name, summary, apply_forget)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcur_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmemory_retrival\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_memory_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/test_json.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index_storage'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_retrival\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_memory_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'index_storage/{user_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-3124628877.py\u001b[0m in \u001b[0;36mload_memory_index\u001b[0;34m(self, vs_path)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_memory_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;34m\"\"\"T·∫£i FAISS index ƒë√£ l∆∞u t·ª´ vs_path\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         vector_store = FAISS.load_local(\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mvs_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36mload_local\u001b[0;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;31m# load index separately since it is not picklable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m         \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependable_faiss_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{index_name}.faiss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \u001b[0;31m# load docstore and index_to_docstore_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/faiss/swigfaiss_avx512.py\u001b[0m in \u001b[0;36mread_index\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m  11639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11640\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11641\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx512\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11643\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open index_storage/Linh/index.faiss for reading: No such file or directory"
          ]
        }
      ],
      "source": [
        "print(test_llm_conversation(query='YOLO:BIT l√† g√¨?', user_name='Linh', firstTime=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OBU8FWDsyP5j"
      },
      "id": "OBU8FWDsyP5j",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}